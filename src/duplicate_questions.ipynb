{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAR Project 2024 - Team Mojave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google colab prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment these lines if you are running the notebook from Google Colab\n",
    "#\n",
    "# !git clone -b feature/sbert https://github.com/opacicmarko/duplicate-question-identification.git\n",
    "# %cd duplicate-question-identification/src/data/\n",
    "# !wget https://sbert.net/datasets/quora-IR-dataset.zip\n",
    "# !unzip quora-IR-dataset.zip\n",
    "# !mv classification/* sbert/\n",
    "# %cd ..\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from utils import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR = './saved_models/'\n",
    "LOG_DIR = './logs/'\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the questions and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_glove_embeddings, load_and_embed_questions, get_max_len\n",
    "\n",
    "# Load embeddings and calculate the average embedding\n",
    "glove_embeddings, glove_avg_embedding = load_glove_embeddings('glove-wikipedia/glove.6B.300d.txt', calculate_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = load_and_embed_questions('data/train.csv', None, glove_embeddings, glove_avg_embedding)\n",
    "print(len(result.vocab.idx_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result.vocab.idx_words))\n",
    "\n",
    "# Sanity check (should look like [-0.36886 0.16665 0.053452 ... -0.030849 -0.031811])\n",
    "print(result.embedding[result.vocab.words_idx['year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating maximum question length...')\n",
    "max_len = get_max_len(pd.read_csv('data/train.csv'))\n",
    "print('Maximum length:', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasplit import make_dataset_split\n",
    "\n",
    "RANDOM_STATE = 73\n",
    "\n",
    "ORIGINAL_DATA_PATH = 'data/train.csv'\n",
    "TRAIN_PATH = 'data/mojave/mojave_train.csv'\n",
    "VALIDATION_PATH = 'data/mojave/mojave_validation.csv'\n",
    "TEST_PATH = 'data/mojave/mojave_test.csv'\n",
    "\n",
    "train_df, valid_df, test_df = make_dataset_split(\n",
    "    data_path=ORIGINAL_DATA_PATH,\n",
    "    train_path=TRAIN_PATH,\n",
    "    validation_path=VALIDATION_PATH,\n",
    "    test_path=TEST_PATH,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.plot.hist(by='is_duplicate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import ProcessedResult, preprocess_text\n",
    "\n",
    "def embedding_for_word(word: str, result: ProcessedResult) -> np.array:\n",
    "    idx = None\n",
    "    idx = result.vocab.words_idx.get(word, None)\n",
    "    if idx is None:\n",
    "        return result.avg_embedding\n",
    "    return result.embedding[idx]\n",
    "\n",
    "def question_embedding_tensor(text: str, result: ProcessedResult, device: torch.device) -> torch.Tensor:\n",
    "    tokens = preprocess_text(text)\n",
    "    token_embeddings = list(map(lambda token: embedding_for_word(token, result), tokens))\n",
    "    if len(token_embeddings) == 0:\n",
    "        token_embeddings = [np.array(result.avg_embedding)]\n",
    "    token_embeddings = np.vstack(token_embeddings)\n",
    "    return torch.from_numpy(token_embeddings.sum(axis=0)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset splits loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from preprocessing import MojaveVocab\n",
    "from typing import Any\n",
    "\n",
    "questions_dtype = {\n",
    "    'id': int,\n",
    "    'qid1': int,\n",
    "    'qid2': int,\n",
    "    'question1': str,\n",
    "    'question2': str,\n",
    "    'is_duplicate': int\n",
    "}\n",
    "\n",
    "class QuestionPairDataset(Dataset):\n",
    "    def __init__(self, questions_path: str, vocab: MojaveVocab, max_len: int, device: torch.device):\n",
    "        self.path = questions_path\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "        self.questions = pd.read_csv(questions_path, dtype=questions_dtype)\n",
    "        # self.result = load_and_embed_questions(questions_path, None, glove_embeddings, glove_avg_embedding)\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        row = self.questions.iloc[index]\n",
    "        question1 = preprocess_text(row['question1'])\n",
    "        question2 = preprocess_text(row['question2'])\n",
    "        label = row['is_duplicate']\n",
    "\n",
    "        question1_idxs = [self.vocab.words_idx.get(word, self.vocab.unk_idx) for word in question1]\n",
    "        question2_idxs = [self.vocab.words_idx.get(word, self.vocab.unk_idx) for word in question2]\n",
    "\n",
    "        question1_idxs = question1_idxs + [self.vocab.pad_idx] * (self.max_len - len(question1_idxs))\n",
    "        question2_idxs = question2_idxs + [self.vocab.pad_idx] * (self.max_len - len(question2_idxs))\n",
    "\n",
    "        return (\n",
    "            torch.tensor(question1_idxs, dtype=torch.long, device=self.device),\n",
    "            torch.tensor(question2_idxs, dtype=torch.long, device=self.device),\n",
    "            torch.tensor(label, dtype=torch.long, device=self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.backends.cuda.is_available() else 'cpu'\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "train_data = QuestionPairDataset(questions_path=TRAIN_PATH, vocab=result.vocab, max_len=max_len, device=device)\n",
    "validation_data = QuestionPairDataset(questions_path=VALIDATION_PATH, vocab=result.vocab, max_len=max_len, device=device)\n",
    "test_data = QuestionPairDataset(questions_path=TEST_PATH, vocab=result.vocab, max_len=max_len, device=device)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import QuestionPairMLP\n",
    "\n",
    "def train_one_epoch(model: QuestionPairMLP, criterion: nn.CrossEntropyLoss, optimizer: torch.optim.Optimizer, training_loader: DataLoader, device: torch.device):\n",
    "    validation_interval = 1000\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    for i, data in enumerate(training_loader):\n",
    "        question1, question2, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(question1, question2)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            log(logger, '  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "        if i % validation_interval == validation_interval - 1:\n",
    "            running_vloss = 0.0\n",
    "            # Set the model to evaluation mode, disabling dropout and using population\n",
    "            # statistics for batch normalization.\n",
    "            model.eval()\n",
    "\n",
    "            # Disable gradient computation and reduce memory consumption.\n",
    "            with torch.no_grad():\n",
    "                for i, vdata in enumerate(validation_dataloader):\n",
    "                    vq1, vq2, vlabels = vdata\n",
    "                    voutputs = model(vq1, vq2)\n",
    "                    vloss = criterion(voutputs, vlabels)\n",
    "                    running_vloss += vloss\n",
    "\n",
    "            avg_vloss = running_vloss / (i + 1)\n",
    "            log(logger, 'LOSS valid {}'.format(avg_vloss))\n",
    "        model.train(True)\n",
    "            \n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_SIZE_1 = 128\n",
    "HIDDEN_LAYER_SIZE_2 = 64\n",
    "EPOCHS = 1\n",
    "LR = 1e-4\n",
    "WD = 0.1\n",
    "\n",
    "model = QuestionPairMLP(len(result.vocab), result.embedding, 300, HIDDEN_LAYER_SIZE_1, HIDDEN_LAYER_SIZE_2, device)\n",
    "model.to(device)\n",
    "\n",
    "# Logging setup\n",
    "timestamp = str(int(time.time()))\n",
    "fh = logging.FileHandler(LOG_DIR + timestamp + '_mlp.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(fh)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "log(logger, 'Run at timestamp: ' + timestamp)\n",
    "log(logger, f'Total params: {sum(parameter.numel() for parameter in model.parameters() if parameter.requires_grad)}')\n",
    "log(logger, f'HIDDEN_LAYER_SIZE_1: {HIDDEN_LAYER_SIZE_1}')\n",
    "log(logger, f'HIDDEN_LAYER_SIZE_2: {HIDDEN_LAYER_SIZE_2}')\n",
    "log(logger, f'LR = {LR}')\n",
    "log(logger, f'WD = {WD}')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    log(logger, f'EPOCH {epoch}')\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(model, criterion, optimizer, train_dataloader, device)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_DIR + timestamp + '.model')\n",
    "\n",
    "print('DONE with training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_tloss = 0.\n",
    "\n",
    "correct_pred = 0\n",
    "total_pred = len(test_data)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(test_dataloader):\n",
    "        tq1, tq2, tlabels = tdata\n",
    "        toutputs = model(tq1, tq2)\n",
    "        prob_toutputs = nn.functional.softmax(toutputs, dim=1)\n",
    "        prediction = torch.zeros_like(prob_toutputs)\n",
    "        mask = toutputs > 0.5\n",
    "        prediction[mask] = 1.\n",
    "        prediction = prediction[:, 0]\n",
    "        correct_pred += int(torch.sum((prediction == tlabels) * (prediction == 1.)).float())\n",
    "        total_pred += prediction.size(0)\n",
    "        tloss = criterion(toutputs, tlabels)\n",
    "        running_tloss += tloss\n",
    "total = i + 1\n",
    "avg_tloss = running_tloss / total\n",
    "accuracy = correct_pred / total_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_tloss\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import QuestionPairCosineSimilarity\n",
    "\n",
    "EPOCHS = 1\n",
    "LR = 1e-5\n",
    "\n",
    "cos_model = QuestionPairCosineSimilarity(len(result.vocab), result.embedding, 300, device)\n",
    "cos_model.to(device)\n",
    "\n",
    "# Logging setup\n",
    "timestamp = str(int(time.time()))\n",
    "fh = logging.FileHandler(LOG_DIR + timestamp + '_cos.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(fh)\n",
    "\n",
    "log(logger, 'Run timestamp: ' + timestamp)\n",
    "log(logger, f'EPOCHS: {EPOCHS}')\n",
    "log(logger, f'LR = {LR}')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cos_model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    cos_model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # Get inputs and labels\n",
    "        q1, q2, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cos_model(q1, q2)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * q1.size(0)\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            log(logger, '  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    running_vloss = 0.\n",
    "    \n",
    "    cos_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_dataloader):\n",
    "            vq1, vq2, vlabels = vdata\n",
    "            voutputs = cos_model(vq1, vq2)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    log(logger, 'LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    log(logger, f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_tloss = 0.\n",
    "\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "cos_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(test_dataloader):\n",
    "        tq1, tq2, tlabels = tdata\n",
    "        toutputs = cos_model(tq1, tq2)\n",
    "        prob_toutputs = nn.functional.softmax(toutputs, dim=1)\n",
    "        prediction = torch.zeros_like(prob_toutputs)\n",
    "        mask = prob_toutputs > 0.5\n",
    "        prediction[mask] = 1.\n",
    "        prediction = prediction[:, 0]\n",
    "        correct_preds += int(torch.sum((prediction == tlabels) * (prediction == 1.)).float())\n",
    "        total_preds += prediction.size(0)\n",
    "        tloss = criterion(toutputs, tlabels)\n",
    "        running_tloss += tloss\n",
    "total = i + 1\n",
    "avg_tloss = running_tloss / total\n",
    "accuracy = correct_preds / total_preds\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# import logging\n",
    "# import time\n",
    "\n",
    "# Suponiendo que result, LOG_DIR, train_dataloader, validation_dataloader y test_dataloader están definidos\n",
    "\n",
    "from models import QuestionPairLSTM\n",
    "\n",
    "EPOCHS = 1\n",
    "LR = 1e-5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lstm_model = QuestionPairLSTM(len(result.vocab), result.embedding, embedding_size=300, hidden_size=128, num_layers=2, device=device)\n",
    "lstm_model.to(device)\n",
    "\n",
    "# Logging setup\n",
    "timestamp = str(int(time.time()))\n",
    "fh = logging.FileHandler(LOG_DIR + timestamp + '_lstm.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(fh)\n",
    "\n",
    "def log(logger, message):\n",
    "    logger.debug(message)\n",
    "    print(message)\n",
    "\n",
    "log(logger, 'Run timestamp: ' + timestamp)\n",
    "log(logger, f'EPOCHS: {EPOCHS}')\n",
    "log(logger, f'LR = {LR}')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    lstm_model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        q1, q2, labels = data\n",
    "        q1, q2, labels = q1.to(device), q2.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(q1, q2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * q1.size(0)\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100\n",
    "            log(logger, f'  batch {i + 1} loss: {last_loss}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_dataloader):\n",
    "            vq1, vq2, vlabels = vdata\n",
    "            vq1, vq2, vlabels = vq1.to(device), vq2.to(device), vlabels.to(device)\n",
    "            voutputs = lstm_model(vq1, vq2)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss.item() * vq1.size(0)\n",
    "\n",
    "    avg_vloss = running_vloss / len(validation_dataloader.dataset)\n",
    "    log(logger, f'LOSS train {running_loss / len(train_dataloader.dataset)} valid {avg_vloss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_tloss = 0.0\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(test_dataloader):\n",
    "        tq1, tq2, tlabels = tdata\n",
    "        tq1, tq2, tlabels = tq1.to(device), tq2.to(device), tlabels.to(device)\n",
    "        toutputs = lstm_model(tq1, tq2)\n",
    "        prob_toutputs = nn.functional.softmax(toutputs, dim=1)\n",
    "        _, predicted = torch.max(prob_toutputs, 1)\n",
    "        correct_preds += (predicted == tlabels).sum().item()\n",
    "        total_preds += tlabels.size(0)\n",
    "        tloss = criterion(toutputs, tlabels)\n",
    "        running_tloss += tloss.item() * tq1.size(0)\n",
    "\n",
    "avg_tloss = running_tloss / len(test_dataloader.dataset)\n",
    "accuracy = correct_preds / total_preds\n",
    "log(logger, f'Test Loss: {avg_tloss}, Test Accuracy: {accuracy}')\n",
    "print(f'Final Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers.readers import InputExample\n",
    "\n",
    "sbert_questions_dtype = {\n",
    "    'qid1': int,\n",
    "    'qid2': int,\n",
    "    'question1': str,\n",
    "    'question2': str,\n",
    "    'is_duplicate': int\n",
    "}\n",
    "\n",
    "class SBERTQuestionPairDataset(Dataset):\n",
    "    def __init__(self, questions_path: str):\n",
    "        self.path = questions_path\n",
    "        self.questions = pd.read_csv(questions_path, sep='\\t', dtype=sbert_questions_dtype, quoting=3)\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    def __getitem__(self, index) -> InputExample:\n",
    "        row = self.questions.iloc[index]\n",
    "        return InputExample(texts=[str(row['question1']), str(row['question2'])], label=int(row['is_duplicate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = SBERTQuestionPairDataset('data/sbert/train_pairs.tsv')\n",
    "dev_dataset = SBERTQuestionPairDataset('data/sbert/dev_pairs.tsv')\n",
    "test_dataset = SBERTQuestionPairDataset('data/sbert/test_pairs.tsv')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilled RoBERTa Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "import math\n",
    "\n",
    "NUM_EPOCHS = 4\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.set_per_process_memory_fraction(0.0)\n",
    "\n",
    "LEARNING_RATES = [3e-5, 4e-5]\n",
    "\n",
    "for lr in LEARNING_RATES:\n",
    "    evaluator = CEBinaryClassificationEvaluator.from_input_examples(list(dev_dataset), name=\"QuoraQuestionPairs-dev\")\n",
    "\n",
    "    warmup_steps = math.ceil(len(train_dataloader) * NUM_EPOCHS * WARMUP_RATIO)\n",
    "    log(logger, \"Warmup-steps: {}\".format(warmup_steps))\n",
    "    log(logger, \"Learing rate: {}\".format(lr))\n",
    "\n",
    "    model = CrossEncoder(\"distilroberta-base\", num_labels=1)\n",
    "\n",
    "    timestamp = str(int(time.time()))\n",
    "\n",
    "    MODEL_SAVE_PATH = MODEL_SAVE_DIR + timestamp + '_' + '{:.0E}'.format(lr) + '/'\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        evaluator=evaluator,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        optimizer_params={'lr': lr},\n",
    "        evaluation_steps=5000,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=MODEL_SAVE_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "\n",
    "model = CrossEncoder(MODEL_SAVE_PATH)\n",
    "\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(list(test_dataset), name=\"QuoraQuestionPairs-test\")\n",
    "evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = SBERTQuestionPairDataset('data/sbert/train_pairs.tsv')\n",
    "dev_dataset = SBERTQuestionPairDataset('data/sbert/dev_pairs.tsv')\n",
    "test_dataset = SBERTQuestionPairDataset('data/sbert/test_pairs.tsv')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "import math\n",
    "\n",
    "NUM_EPOCHS = 4\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.set_per_process_memory_fraction(0.0)\n",
    "\n",
    "LEARNING_RATES = [3e-5, 4e-5]\n",
    "\n",
    "for lr in LEARNING_RATES:\n",
    "    evaluator = CEBinaryClassificationEvaluator.from_input_examples(list(dev_dataset), name=\"QuoraQuestionPairs-dev\")\n",
    "\n",
    "    warmup_steps = math.ceil(len(train_dataloader) * NUM_EPOCHS * WARMUP_RATIO)\n",
    "    log(logger, \"Warmup-steps: {}\".format(warmup_steps))\n",
    "    log(logger, \"Learing rate: {}\".format(lr))\n",
    "\n",
    "    model = CrossEncoder(\"roberta-base\", num_labels=1)\n",
    "\n",
    "    timestamp = str(int(time.time()))\n",
    "\n",
    "    MODEL_SAVE_PATH = MODEL_SAVE_DIR + timestamp + '_' + '{:.0E}'.format(lr) + '/'\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        evaluator=evaluator,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        optimizer_params={'lr': lr},\n",
    "        evaluation_steps=5000,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=MODEL_SAVE_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "\n",
    "model = CrossEncoder(MODEL_SAVE_PATH)\n",
    "\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(list(test_dataset), name=\"QuoraQuestionPairs-test\")\n",
    "evaluator(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
