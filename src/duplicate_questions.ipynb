{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAR Project 2024 - Team Mojave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from utils import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR = './saved_models/'\n",
    "LOG_DIR = './logs/'\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the questions and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_glove_embeddings, load_and_embed_questions, get_max_len\n",
    "\n",
    "# Load embeddings and calculate the average embedding\n",
    "glove_embeddings, glove_avg_embedding = load_glove_embeddings('glove-wikipedia/glove.6B.300d.txt', calculate_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...DONE\n",
      "Generating vocabulary...DONE\n",
      "Extracting only embeddings required for vocabulary...DONE\n",
      "6662\n"
     ]
    }
   ],
   "source": [
    "result = load_and_embed_questions('data/train.csv', None, glove_embeddings, glove_avg_embedding)\n",
    "print(len(result.vocab.idx_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6662\n",
      "[-3.6886e-01  1.6665e-01  5.3452e-02  3.1654e-01 -1.5587e-01  3.7323e-01\n",
      "  2.3476e-02 -7.7873e-02 -1.2866e-01 -1.4157e+00  4.2323e-01  1.0872e-01\n",
      "  1.9566e-01 -3.6958e-02 -3.1716e-01  3.2313e-02 -1.0706e-01  1.5411e-01\n",
      " -1.1385e-01 -5.3492e-01 -8.8111e-02 -8.2342e-02 -1.1802e-02  5.7562e-02\n",
      " -2.5138e-01 -2.3305e-01 -1.4013e-01 -2.9804e-01  4.2330e-01  1.4148e-01\n",
      "  2.1174e-01  3.0089e-03 -7.2124e-02  1.0726e-01 -1.4072e+00 -3.6393e-01\n",
      " -7.0648e-02 -3.5023e-02  9.2109e-03 -4.6376e-03  3.6941e-01 -3.1950e-01\n",
      " -4.0905e-01  8.1857e-02 -2.7848e-01 -2.3384e-01 -3.5798e-01  4.4094e-01\n",
      " -1.9289e-01  1.8111e-01  1.8372e-01 -1.7586e-01 -8.4658e-02  2.7628e-02\n",
      "  2.4343e-01  2.2452e-01  9.2825e-02  1.8590e-01 -1.6946e-01 -2.1119e-01\n",
      " -4.8065e-01  2.3327e-02  3.2214e-01 -4.8864e-01 -4.5548e-02 -6.6230e-01\n",
      " -8.0392e-02  1.7008e-01 -8.6406e-02 -1.7071e-01  3.8116e-02  3.6805e-01\n",
      " -9.9032e-02  1.8643e-01 -2.5289e-02 -3.4840e-01  2.6928e-01 -4.0263e-01\n",
      " -4.1668e-01  4.0442e-01 -3.0187e-03 -1.2581e-01  1.8271e-01  3.7474e-01\n",
      "  5.3416e-01  2.0162e-01 -3.3906e-01  2.2156e-01 -2.9287e-03  2.0424e-01\n",
      " -9.7543e-04 -4.0551e-02 -4.5608e-01 -4.3217e-02 -3.1431e-01  1.3197e-01\n",
      " -2.8520e-01  1.4870e-01 -1.1564e-01 -3.4639e-02 -4.8005e-02  9.5907e-02\n",
      "  8.2946e-02 -9.0491e-02 -3.1543e-01 -3.6012e-01 -1.8588e-01  3.1254e-01\n",
      " -4.6717e-01 -1.1390e-01 -1.6014e-01 -3.3473e-01 -5.0582e-01 -4.4279e-01\n",
      " -1.1597e-01 -1.9440e-01  1.0958e-01  8.9686e-02  1.9664e-01 -3.3752e-01\n",
      " -2.3088e-01 -2.8500e-01 -1.4345e-01  3.4356e-01  3.7536e-01  1.7423e-01\n",
      "  9.5387e-02  1.2255e-01  3.7534e-01  1.9590e-01 -2.0763e-01  4.2285e-01\n",
      "  4.7908e-02  2.8730e-03 -1.0248e-01 -1.0990e-02  3.0316e-02  3.0923e-01\n",
      " -7.8191e-02  1.7161e-01  1.2297e-01  1.1334e-01  5.8435e-02  7.7761e-02\n",
      " -4.3548e-01  1.9657e-01 -1.8545e-01  3.0334e-02  2.0523e-01  2.3109e-01\n",
      "  7.0326e-01 -6.1502e-02 -2.8793e-02  3.0059e-01  2.2463e-01 -2.3505e-01\n",
      " -8.8845e-01 -2.2767e-02 -4.9855e-01 -1.7016e-01 -1.3950e-01  2.5470e-02\n",
      " -1.6716e-01  1.5786e-01 -1.6651e-04 -2.8056e-02  1.3545e-01 -5.9520e-02\n",
      "  1.3900e-01  3.7920e-01  2.1455e-01  5.5587e-01 -6.4773e-01  3.0938e-02\n",
      " -2.2821e-02  1.7538e-01  1.4959e-01 -2.3300e-01 -1.7293e-01  1.0277e+00\n",
      "  1.7895e-01  3.0013e-01  1.7709e-02  8.8489e-02  3.7098e-01 -1.0803e-01\n",
      " -4.2111e-01  3.9310e-01 -2.1920e-01  2.0519e-01 -5.2980e-01 -2.5640e-01\n",
      " -1.1884e-01 -1.4738e-01  3.1549e-01 -1.3869e-01 -8.3845e-02  2.7897e-01\n",
      "  3.7843e-01 -1.1851e-02  1.2435e+00 -2.7210e-02 -1.6494e-01  6.8978e-02\n",
      " -1.4388e-02  1.0964e-01 -9.2518e-03  4.1301e-01  2.3176e-02 -2.7716e-01\n",
      "  2.2952e-01 -1.2557e-01  4.1034e-01 -2.4484e-01  9.3694e-02 -4.0638e-02\n",
      " -2.2682e-01  3.5462e-01 -5.9728e-02 -7.6283e-02  7.2801e-01 -2.3714e-01\n",
      " -5.9137e-01  8.4563e-01  2.3823e-02  1.3666e-01  1.8534e-01 -1.6146e-01\n",
      "  1.8006e-01  3.2675e-01 -4.4427e-02  2.7968e-02 -7.9990e-02 -4.3421e-01\n",
      "  3.0875e-02  3.0228e-02  4.8839e-01 -3.4265e-02  4.2164e-01 -1.5090e-01\n",
      " -2.2167e-01 -2.8621e-01  4.5150e-01  1.2549e-01 -1.0057e+00 -1.2271e-02\n",
      "  6.3056e-01 -6.5727e-02  7.7733e-03  3.9281e-02  1.1555e-01  3.0901e-02\n",
      " -1.9538e-02 -2.2340e-01  2.3992e-01 -2.4529e-01  3.1603e-01  2.1532e-01\n",
      "  4.0016e-01  1.5882e-01 -4.4926e-02  2.6241e-01 -3.0474e-02 -1.1219e-01\n",
      "  2.8185e-01 -1.3680e-02  1.8972e-01  3.2951e-02  3.6970e-01 -6.7917e-02\n",
      "  2.0682e-01  2.5769e-02  2.4305e-01  3.8442e-01  5.5733e-01 -2.9742e-01\n",
      " -2.2098e+00 -2.4059e-01  1.1628e-01  4.0257e-01 -6.1286e-01 -1.9978e-01\n",
      " -1.1517e-01  3.6771e-02 -1.4610e-01  7.3904e-01 -3.5831e-01 -9.7467e-02\n",
      " -4.6618e-01 -2.9867e-01  1.4423e-01  2.5053e-02  1.3952e-01  2.1374e-01\n",
      "  3.4057e-01  4.7053e-01  6.9752e-03 -2.4409e-01 -3.0849e-02 -3.1811e-02]\n"
     ]
    }
   ],
   "source": [
    "print(len(result.vocab.idx_words))\n",
    "\n",
    "# Sanity check (should look like [-0.36886 0.16665 0.053452 ... -0.030849 -0.031811])\n",
    "print(result.embedding[result.vocab.words_idx['year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating maximum question length...\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "Maximum length: 89\n"
     ]
    }
   ],
   "source": [
    "print('Calculating maximum question length...')\n",
    "max_len = get_max_len(pd.read_csv('data/train.csv'))\n",
    "print('Maximum length:', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Use 20% of data for the test set\n",
    "train_df, test_df = train_test_split(data, test_size=0.2)\n",
    "# Use 1/8 of the remaining for a 70/10/20 train/validation/test split\n",
    "train_df, validation_df = train_test_split(train_df, test_size=0.125)\n",
    "\n",
    "TRAIN_PATH = 'data/mojave/mojave_train.csv'\n",
    "VALIDATION_PATH = 'data/mojave/mojave_validation.csv'\n",
    "TEST_PATH = 'data/mojave/mojave_test.csv'\n",
    "\n",
    "train_df.to_csv(TRAIN_PATH)\n",
    "validation_df.to_csv(VALIDATION_PATH)\n",
    "test_df.to_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.plot.hist(by='is_duplicate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import ProcessedResult, preprocess_text\n",
    "\n",
    "def embedding_for_word(word: str, result: ProcessedResult) -> np.array:\n",
    "    idx = None\n",
    "    idx = result.vocab.words_idx.get(word, None)\n",
    "    if idx is None:\n",
    "        return result.avg_embedding\n",
    "    return result.embedding[idx]\n",
    "\n",
    "def question_embedding_tensor(text: str, result: ProcessedResult, device: torch.device) -> torch.Tensor:\n",
    "    tokens = preprocess_text(text)\n",
    "    token_embeddings = list(map(lambda token: embedding_for_word(token, result), tokens))\n",
    "    if len(token_embeddings) == 0:\n",
    "        token_embeddings = [np.array(result.avg_embedding)]\n",
    "    token_embeddings = np.vstack(token_embeddings)\n",
    "    return torch.from_numpy(token_embeddings.sum(axis=0)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from preprocessing import MojaveVocab\n",
    "from typing import Any\n",
    "\n",
    "questions_dtype = {\n",
    "    'id': int,\n",
    "    'qid1': int,\n",
    "    'qid2': int,\n",
    "    'question1': str,\n",
    "    'question2': str,\n",
    "    'is_duplicate': int\n",
    "}\n",
    "\n",
    "class QuestionPairDataset(Dataset):\n",
    "    def __init__(self, questions_path: str, vocab: MojaveVocab, max_len: int, device: torch.device):\n",
    "        self.path = questions_path\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "        self.questions = pd.read_csv(questions_path, dtype=questions_dtype)\n",
    "        # self.result = load_and_embed_questions(questions_path, None, glove_embeddings, glove_avg_embedding)\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        row = self.questions.iloc[index]\n",
    "        question1 = preprocess_text(row['question1'])\n",
    "        question2 = preprocess_text(row['question2'])\n",
    "        label = row['is_duplicate']\n",
    "\n",
    "        question1_idxs = [self.vocab.words_idx.get(word, self.vocab.unk_idx) for word in question1]\n",
    "        question2_idxs = [self.vocab.words_idx.get(word, self.vocab.unk_idx) for word in question2]\n",
    "\n",
    "        question1_idxs = question1_idxs + [self.vocab.pad_idx] * (self.max_len - len(question1_idxs))\n",
    "        question2_idxs = question2_idxs + [self.vocab.pad_idx] * (self.max_len - len(question2_idxs))\n",
    "\n",
    "        return (\n",
    "            torch.tensor(question1_idxs, dtype=torch.long, device=self.device),\n",
    "            torch.tensor(question2_idxs, dtype=torch.long, device=self.device),\n",
    "            torch.tensor(label, dtype=torch.long, device=self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.backends.cuda.is_available() else 'cpu'\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "train_data = QuestionPairDataset(questions_path=TRAIN_PATH, vocab=result.vocab, max_len=max_len, device=device)\n",
    "validation_data = QuestionPairDataset(questions_path=VALIDATION_PATH, vocab=result.vocab, max_len=max_len, device=device)\n",
    "test_data = QuestionPairDataset(questions_path=TEST_PATH, vocab=result.vocab, max_len=max_len, device=device)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import QuestionPairMLP\n",
    "\n",
    "def train_one_epoch(model: QuestionPairMLP, criterion: nn.CrossEntropyLoss, optimizer: torch.optim.Optimizer, training_loader: DataLoader, device: torch.device):\n",
    "    validation_interval = 1000\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    for i, data in enumerate(training_loader):\n",
    "        question1, question2, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(question1, question2)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            log(logger, '  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "        if i % validation_interval == validation_interval - 1:\n",
    "            running_vloss = 0.0\n",
    "            # Set the model to evaluation mode, disabling dropout and using population\n",
    "            # statistics for batch normalization.\n",
    "            model.eval()\n",
    "\n",
    "            # Disable gradient computation and reduce memory consumption.\n",
    "            with torch.no_grad():\n",
    "                for i, vdata in enumerate(validation_dataloader):\n",
    "                    vq1, vq2, vlabels = vdata\n",
    "                    voutputs = model(vq1, vq2)\n",
    "                    vloss = criterion(voutputs, vlabels)\n",
    "                    running_vloss += vloss\n",
    "\n",
    "            avg_vloss = running_vloss / (i + 1)\n",
    "            log(logger, 'LOSS valid {}'.format(avg_vloss))\n",
    "        model.train(True)\n",
    "            \n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run at timestamp: 1716733115\n",
      "Total params: 85314\n",
      "HIDDEN_LAYER_SIZE_1: 128\n",
      "HIDDEN_LAYER_SIZE_2: 64\n",
      "LR = 0.0001\n",
      "WD = 0.1\n",
      "EPOCH 0\n",
      "  batch 100 loss: 0.6761941587924958\n",
      "  batch 200 loss: 0.6782734596729278\n",
      "  batch 300 loss: 0.6793051534891128\n",
      "  batch 400 loss: 0.6770786172151566\n",
      "  batch 500 loss: 0.6803968930244446\n",
      "  batch 600 loss: 0.6764743041992187\n",
      "  batch 700 loss: 0.677337054014206\n",
      "  batch 800 loss: 0.6756083422899246\n",
      "  batch 900 loss: 0.6778529155254364\n",
      "  batch 1000 loss: 0.6790100526809693\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.67635178565979\n",
      "  batch 1100 loss: 0.6800547695159912\n",
      "  batch 1200 loss: 0.6726067370176315\n",
      "  batch 1300 loss: 0.6738550990819931\n",
      "  batch 1400 loss: 0.6764506411552429\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "  batch 1500 loss: 0.674996309876442\n",
      "  batch 1600 loss: 0.6741819626092911\n",
      "  batch 1700 loss: 0.6740669453144074\n",
      "  batch 1800 loss: 0.6773726505041122\n",
      "  batch 1900 loss: 0.6780846983194351\n",
      "  batch 2000 loss: 0.6733337545394897\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6744663715362549\n",
      "  batch 2100 loss: 0.675062472820282\n",
      "  batch 2200 loss: 0.6755480182170868\n",
      "  batch 2300 loss: 0.6736624652147293\n",
      "  batch 2400 loss: 0.6722759133577347\n",
      "  batch 2500 loss: 0.6725102186203002\n",
      "  batch 2600 loss: 0.6712711381912232\n",
      "  batch 2700 loss: 0.6741603910923004\n",
      "  batch 2800 loss: 0.671235078573227\n",
      "  batch 2900 loss: 0.6721969252824783\n",
      "  batch 3000 loss: 0.6755654364824295\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6727758049964905\n",
      "  batch 3100 loss: 0.6752700459957123\n",
      "  batch 3200 loss: 0.6716873753070831\n",
      "  batch 3300 loss: 0.6717151767015457\n",
      "  batch 3400 loss: 0.6758524966239929\n",
      "  batch 3500 loss: 0.675411605834961\n",
      "  batch 3600 loss: 0.6752880698442459\n",
      "  batch 3700 loss: 0.6703368014097214\n",
      "  batch 3800 loss: 0.6758404719829559\n",
      "  batch 3900 loss: 0.6714435255527497\n",
      "  batch 4000 loss: 0.6751362770795822\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6714862585067749\n",
      "  batch 4100 loss: 0.6732707089185714\n",
      "  batch 4200 loss: 0.6745954090356827\n",
      "  batch 4300 loss: 0.6719033545255662\n",
      "  batch 4400 loss: 0.6686241567134857\n",
      "  batch 4500 loss: 0.6718905109167099\n",
      "  batch 4600 loss: 0.6727284896373749\n",
      "  batch 4700 loss: 0.6704559922218323\n",
      "  batch 4800 loss: 0.6681615829467773\n",
      "  batch 4900 loss: 0.6682741272449494\n",
      "  batch 5000 loss: 0.6667437100410462\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6701774001121521\n",
      "  batch 5100 loss: 0.671821762919426\n",
      "  batch 5200 loss: 0.6710418713092804\n",
      "  batch 5300 loss: 0.666299592256546\n",
      "  batch 5400 loss: 0.6704007518291474\n",
      "  batch 5500 loss: 0.6746987062692642\n",
      "  batch 5600 loss: 0.6651913160085678\n",
      "  batch 5700 loss: 0.6735317659378052\n",
      "  batch 5800 loss: 0.6691010755300522\n",
      "  batch 5900 loss: 0.6694383955001831\n",
      "  batch 6000 loss: 0.6668335288763046\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6690667271614075\n",
      "  batch 6100 loss: 0.66983118891716\n",
      "  batch 6200 loss: 0.6671609073877335\n",
      "  batch 6300 loss: 0.6670674771070481\n",
      "  batch 6400 loss: 0.669801470041275\n",
      "  batch 6500 loss: 0.6694205832481385\n",
      "  batch 6600 loss: 0.666103469133377\n",
      "  batch 6700 loss: 0.6629170906543732\n",
      "  batch 6800 loss: 0.6639631718397141\n",
      "  batch 6900 loss: 0.6681933516263961\n",
      "  batch 7000 loss: 0.6712328791618347\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6680063009262085\n",
      "  batch 7100 loss: 0.6669083219766617\n",
      "  batch 7200 loss: 0.6682381319999695\n",
      "  batch 7300 loss: 0.66974001288414\n",
      "  batch 7400 loss: 0.6671239423751831\n",
      "  batch 7500 loss: 0.669752242565155\n",
      "  batch 7600 loss: 0.6674215489625931\n",
      "  batch 7700 loss: 0.6712344837188721\n",
      "  batch 7800 loss: 0.6658094298839569\n",
      "  batch 7900 loss: 0.6679955917596817\n",
      "  batch 8000 loss: 0.6605110496282578\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6671226620674133\n",
      "  batch 8100 loss: 0.6673219764232635\n",
      "  batch 8200 loss: 0.6672303622961044\n",
      "  batch 8300 loss: 0.670825662612915\n",
      "  batch 8400 loss: 0.6716320520639419\n",
      "  batch 8500 loss: 0.6685687017440796\n",
      "  batch 8600 loss: 0.6703472995758056\n",
      "  batch 8700 loss: 0.6662319606542587\n",
      "  batch 8800 loss: 0.6651443940401077\n",
      "  batch 8900 loss: 0.6576725697517395\n",
      "  batch 9000 loss: 0.6673442900180817\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6663689613342285\n",
      "  batch 9100 loss: 0.6650218826532364\n",
      "  batch 9200 loss: 0.6697871041297913\n",
      "  batch 9300 loss: 0.6653816193342209\n",
      "  batch 9400 loss: 0.6628870970010757\n",
      "  batch 9500 loss: 0.6668062949180603\n",
      "  batch 9600 loss: 0.6679829978942871\n",
      "  batch 9700 loss: 0.6697051310539246\n",
      "  batch 9800 loss: 0.6632736575603485\n",
      "  batch 9900 loss: 0.666380888223648\n",
      "  batch 10000 loss: 0.6702668309211731\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.665717601776123\n",
      "  batch 10100 loss: 0.6634306597709656\n",
      "  batch 10200 loss: 0.664413338303566\n",
      "  batch 10300 loss: 0.6685180431604385\n",
      "  batch 10400 loss: 0.6679182833433152\n",
      "  batch 10500 loss: 0.6618445301055909\n",
      "  batch 10600 loss: 0.6610286253690719\n",
      "  batch 10700 loss: 0.6739817881584167\n",
      "  batch 10800 loss: 0.6636407476663589\n",
      "  batch 10900 loss: 0.6685829496383667\n",
      "  batch 11000 loss: 0.6661376112699509\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6651231646537781\n",
      "  batch 11100 loss: 0.6646259462833405\n",
      "  batch 11200 loss: 0.6660140228271484\n",
      "  batch 11300 loss: 0.6648433339595795\n",
      "  batch 11400 loss: 0.6642376345396042\n",
      "  batch 11500 loss: 0.6643737810850143\n",
      "  batch 11600 loss: 0.6648676282167435\n",
      "  batch 11700 loss: 0.6640570306777954\n",
      "  batch 11800 loss: 0.665519363284111\n",
      "  batch 11900 loss: 0.6683212047815323\n",
      "  batch 12000 loss: 0.6662021964788437\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6645748615264893\n",
      "  batch 12100 loss: 0.6673074156045914\n",
      "  batch 12200 loss: 0.6707444697618484\n",
      "  batch 12300 loss: 0.6684182798862457\n",
      "  batch 12400 loss: 0.6629606068134308\n",
      "  batch 12500 loss: 0.6685315907001496\n",
      "  batch 12600 loss: 0.6692826753854751\n",
      "  batch 12700 loss: 0.6640067571401596\n",
      "  batch 12800 loss: 0.6678712749481202\n",
      "  batch 12900 loss: 0.6686010348796845\n",
      "  batch 13000 loss: 0.6590075159072876\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6641659140586853\n",
      "  batch 13100 loss: 0.660093652009964\n",
      "  batch 13200 loss: 0.6598232120275498\n",
      "  batch 13300 loss: 0.6601280349493027\n",
      "  batch 13400 loss: 0.6602670150995255\n",
      "  batch 13500 loss: 0.6599772453308106\n",
      "  batch 13600 loss: 0.6615048795938492\n",
      "  batch 13700 loss: 0.6606446248292923\n",
      "  batch 13800 loss: 0.6680351901054382\n",
      "  batch 13900 loss: 0.6706520223617554\n",
      "  batch 14000 loss: 0.6718250584602355\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6636931300163269\n",
      "  batch 14100 loss: 0.6629411005973815\n",
      "  batch 14200 loss: 0.6663492947816849\n",
      "  batch 14300 loss: 0.664284166097641\n",
      "  batch 14400 loss: 0.6603764283657074\n",
      "  batch 14500 loss: 0.6680550014972687\n",
      "  batch 14600 loss: 0.668261656165123\n",
      "  batch 14700 loss: 0.6674140858650207\n",
      "  batch 14800 loss: 0.6692439883947372\n",
      "  batch 14900 loss: 0.6657367187738419\n",
      "  batch 15000 loss: 0.6583159220218658\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6633456349372864\n",
      "  batch 15100 loss: 0.662783260345459\n",
      "  batch 15200 loss: 0.6643996959924698\n",
      "  batch 15300 loss: 0.6610603660345078\n",
      "  batch 15400 loss: 0.6605712777376175\n",
      "  batch 15500 loss: 0.6655290216207504\n",
      "  batch 15600 loss: 0.6542170590162277\n",
      "  batch 15700 loss: 0.6612506258487701\n",
      "  batch 15800 loss: 0.6605634337663651\n",
      "  batch 15900 loss: 0.6594802814722062\n",
      "  batch 16000 loss: 0.6606751644611358\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6629070043563843\n",
      "  batch 16100 loss: 0.6617107164859771\n",
      "  batch 16200 loss: 0.6591103744506835\n",
      "  batch 16300 loss: 0.665234904885292\n",
      "  batch 16400 loss: 0.6677799797058106\n",
      "  batch 16500 loss: 0.662412760257721\n",
      "  batch 16600 loss: 0.6673054671287537\n",
      "  batch 16700 loss: 0.656565037369728\n",
      "  batch 16800 loss: 0.6618994039297104\n",
      "  batch 16900 loss: 0.6579755955934524\n",
      "  batch 17000 loss: 0.6652549427747726\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.662572979927063\n",
      "  batch 17100 loss: 0.6641696763038635\n",
      "  batch 17200 loss: 0.6645646953582763\n",
      "  batch 17300 loss: 0.660861748456955\n",
      "  batch 17400 loss: 0.6623380589485168\n",
      "  batch 17500 loss: 0.6618810623884201\n",
      "  batch 17600 loss: 0.6585645931959152\n",
      "DONE with training\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_LAYER_SIZE_1 = 128\n",
    "HIDDEN_LAYER_SIZE_2 = 64\n",
    "EPOCHS = 1\n",
    "LR = 1e-4\n",
    "WD = 0.1\n",
    "\n",
    "model = QuestionPairMLP(len(result.vocab), result.embedding, 300, HIDDEN_LAYER_SIZE_1, HIDDEN_LAYER_SIZE_2, device)\n",
    "model.to(device)\n",
    "\n",
    "# Logging setup\n",
    "timestamp = str(int(time.time()))\n",
    "fh = logging.FileHandler(LOG_DIR + timestamp + '_mlp.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(fh)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "log(logger, 'Run at timestamp: ' + timestamp)\n",
    "log(logger, f'Total params: {sum(parameter.numel() for parameter in model.parameters() if parameter.requires_grad)}')\n",
    "log(logger, f'HIDDEN_LAYER_SIZE_1: {HIDDEN_LAYER_SIZE_1}')\n",
    "log(logger, f'HIDDEN_LAYER_SIZE_2: {HIDDEN_LAYER_SIZE_2}')\n",
    "log(logger, f'LR = {LR}')\n",
    "log(logger, f'WD = {WD}')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    log(logger, f'EPOCH {epoch}')\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(model, criterion, optimizer, train_dataloader, device)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_DIR + timestamp + '.model')\n",
    "\n",
    "print('DONE with training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text parameter is not string, but <class 'float'>: nan\n"
     ]
    }
   ],
   "source": [
    "running_tloss = 0.\n",
    "\n",
    "correct_pred = 0\n",
    "total_pred = len(test_data)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(test_dataloader):\n",
    "        tq1, tq2, tlabels = tdata\n",
    "        toutputs = model(tq1, tq2)\n",
    "        prob_toutputs = nn.functional.softmax(toutputs, dim=1)\n",
    "        prediction = torch.zeros_like(prob_toutputs)\n",
    "        mask = toutputs > 0.5\n",
    "        prediction[mask] = 1.\n",
    "        prediction = prediction[:, 0]\n",
    "        correct_pred += int(torch.sum((prediction == tlabels) * (prediction == 1.)).float())\n",
    "        total_pred += prediction.size(0)\n",
    "        tloss = criterion(toutputs, tlabels)\n",
    "        running_tloss += tloss\n",
    "total = i + 1\n",
    "avg_tloss = running_tloss / total\n",
    "accuracy = correct_pred / total_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg_tloss\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run timestamp: 1716734587\n",
      "EPOCHS: 1\n",
      "LR = 1e-05\n",
      "  batch 100 loss: 13.125542163848877\n",
      "  batch 200 loss: 13.084531574249267\n",
      "  batch 300 loss: 13.01950047492981\n",
      "  batch 400 loss: 13.44168363571167\n",
      "  batch 500 loss: 13.15376296043396\n",
      "  batch 600 loss: 13.1065620803833\n",
      "  batch 700 loss: 13.172521514892578\n",
      "  batch 800 loss: 13.176909551620483\n",
      "  batch 900 loss: 13.132677526474\n",
      "  batch 1000 loss: 13.117112083435059\n",
      "  batch 1100 loss: 13.265741348266602\n",
      "  batch 1200 loss: 13.09733465194702\n",
      "  batch 1300 loss: 13.094244260787963\n",
      "  batch 1400 loss: 13.01409460067749\n",
      "  batch 1500 loss: 13.022502641677857\n",
      "  batch 1600 loss: 12.835916185379029\n",
      "  batch 1700 loss: 13.016829757690429\n",
      "  batch 1800 loss: 13.062206964492798\n",
      "  batch 1900 loss: 13.228605394363404\n",
      "  batch 2000 loss: 13.041410112380982\n",
      "  batch 2100 loss: 13.02677752494812\n",
      "  batch 2200 loss: 12.912434816360474\n",
      "  batch 2300 loss: 12.917502946853638\n",
      "  batch 2400 loss: 13.07784942626953\n",
      "  batch 2500 loss: 13.19842155456543\n",
      "  batch 2600 loss: 12.909504547119141\n",
      "  batch 2700 loss: 12.92250415802002\n",
      "  batch 2800 loss: 13.098742179870605\n",
      "  batch 2900 loss: 12.966028289794922\n",
      "  batch 3000 loss: 13.236049499511719\n",
      "  batch 3100 loss: 12.959516630172729\n",
      "  batch 3200 loss: 13.02415584564209\n",
      "  batch 3300 loss: 12.911162433624268\n",
      "  batch 3400 loss: 13.045729312896729\n",
      "  batch 3500 loss: 12.864895057678222\n",
      "  batch 3600 loss: 12.865869131088257\n",
      "  batch 3700 loss: 12.778618173599243\n",
      "  batch 3800 loss: 12.778739309310913\n",
      "  batch 3900 loss: 12.910044946670531\n",
      "  batch 4000 loss: 12.926288213729858\n",
      "  batch 4100 loss: 12.915350818634034\n",
      "  batch 4200 loss: 12.770366973876953\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "  batch 4300 loss: 12.928938083648681\n",
      "  batch 4400 loss: 12.777683477401734\n",
      "  batch 4500 loss: 12.897811460494996\n",
      "  batch 4600 loss: 12.98119213104248\n",
      "  batch 4700 loss: 12.9072332572937\n",
      "  batch 4800 loss: 12.911561145782471\n",
      "  batch 4900 loss: 12.793798608779907\n",
      "  batch 5000 loss: 12.791119108200073\n",
      "  batch 5100 loss: 12.87044140815735\n",
      "  batch 5200 loss: 12.798771572113036\n",
      "  batch 5300 loss: 12.69327223777771\n",
      "  batch 5400 loss: 12.685161533355712\n",
      "  batch 5500 loss: 12.758810148239135\n",
      "  batch 5600 loss: 12.851711902618408\n",
      "  batch 5700 loss: 12.870542573928834\n",
      "  batch 5800 loss: 12.938449869155884\n",
      "  batch 5900 loss: 12.691681089401245\n",
      "  batch 6000 loss: 12.786294708251953\n",
      "  batch 6100 loss: 12.870909509658814\n",
      "  batch 6200 loss: 12.717987718582153\n",
      "  batch 6300 loss: 12.772410459518433\n",
      "  batch 6400 loss: 12.684014940261841\n",
      "  batch 6500 loss: 12.791578168869018\n",
      "  batch 6600 loss: 12.883257083892822\n",
      "  batch 6700 loss: 12.781135540008545\n",
      "  batch 6800 loss: 12.757438707351685\n",
      "  batch 6900 loss: 12.785095777511597\n",
      "  batch 7000 loss: 12.699850521087647\n",
      "  batch 7100 loss: 12.729167642593383\n",
      "  batch 7200 loss: 12.919949750900269\n",
      "  batch 7300 loss: 13.009974575042724\n",
      "  batch 7400 loss: 12.807009258270263\n",
      "  batch 7500 loss: 12.803487005233764\n",
      "  batch 7600 loss: 12.845147180557252\n",
      "  batch 7700 loss: 12.722933931350708\n",
      "  batch 7800 loss: 12.673867263793944\n",
      "  batch 7900 loss: 12.706573543548584\n",
      "  batch 8000 loss: 12.745581884384155\n",
      "  batch 8100 loss: 12.746311597824096\n",
      "  batch 8200 loss: 12.639630126953126\n",
      "  batch 8300 loss: 12.780764141082763\n",
      "  batch 8400 loss: 12.653295783996581\n",
      "  batch 8500 loss: 12.6042138671875\n",
      "  batch 8600 loss: 12.577718925476074\n",
      "  batch 8700 loss: 12.738542127609254\n",
      "  batch 8800 loss: 12.616404151916504\n",
      "  batch 8900 loss: 12.655012826919556\n",
      "  batch 9000 loss: 12.58242247581482\n",
      "  batch 9100 loss: 12.625362596511842\n",
      "  batch 9200 loss: 12.677927436828613\n",
      "  batch 9300 loss: 12.41597785949707\n",
      "  batch 9400 loss: 12.493468608856201\n",
      "  batch 9500 loss: 12.684014625549317\n",
      "  batch 9600 loss: 12.61446816444397\n",
      "  batch 9700 loss: 12.729063606262207\n",
      "  batch 9800 loss: 12.592914056777953\n",
      "  batch 9900 loss: 12.609777736663819\n",
      "  batch 10000 loss: 12.49106580734253\n",
      "  batch 10100 loss: 12.607328453063964\n",
      "  batch 10200 loss: 12.515122451782226\n",
      "  batch 10300 loss: 12.447818155288696\n",
      "  batch 10400 loss: 12.586279544830322\n",
      "  batch 10500 loss: 12.71947639465332\n",
      "  batch 10600 loss: 12.64928568840027\n",
      "  batch 10700 loss: 12.688489255905152\n",
      "  batch 10800 loss: 12.581412200927735\n",
      "  batch 10900 loss: 12.632698793411254\n",
      "  batch 11000 loss: 12.58159670829773\n",
      "  batch 11100 loss: 12.680947494506835\n",
      "  batch 11200 loss: 12.689563436508179\n",
      "  batch 11300 loss: 12.506831483840942\n",
      "  batch 11400 loss: 12.518534460067748\n",
      "  batch 11500 loss: 12.538668146133423\n",
      "  batch 11600 loss: 12.524470901489257\n",
      "  batch 11700 loss: 12.284627256393433\n",
      "  batch 11800 loss: 12.50344488143921\n",
      "  batch 11900 loss: 12.588345994949341\n",
      "  batch 12000 loss: 12.64077838897705\n",
      "  batch 12100 loss: 12.466832647323608\n",
      "  batch 12200 loss: 12.489009971618652\n",
      "  batch 12300 loss: 12.596124305725098\n",
      "  batch 12400 loss: 12.566858625411987\n",
      "  batch 12500 loss: 12.353911380767823\n",
      "  batch 12600 loss: 12.54045599937439\n",
      "  batch 12700 loss: 12.525763549804687\n",
      "  batch 12800 loss: 12.449924564361572\n",
      "  batch 12900 loss: 12.518759031295776\n",
      "  batch 13000 loss: 12.561452808380126\n",
      "  batch 13100 loss: 12.589726085662841\n",
      "  batch 13200 loss: 12.337389526367188\n",
      "  batch 13300 loss: 12.566959819793702\n",
      "  batch 13400 loss: 12.57543249130249\n",
      "  batch 13500 loss: 12.644379444122315\n",
      "  batch 13600 loss: 12.449274854660034\n",
      "  batch 13700 loss: 12.76632761001587\n",
      "  batch 13800 loss: 12.667159872055054\n",
      "  batch 13900 loss: 12.568641729354859\n",
      "  batch 14000 loss: 12.607402572631836\n",
      "  batch 14100 loss: 12.361354942321777\n",
      "  batch 14200 loss: 12.615060539245606\n",
      "  batch 14300 loss: 12.485607042312623\n",
      "  batch 14400 loss: 12.520207796096802\n",
      "  batch 14500 loss: 12.423586263656617\n",
      "  batch 14600 loss: 12.506347589492798\n",
      "  batch 14700 loss: 12.519100894927979\n",
      "  batch 14800 loss: 12.316375532150268\n",
      "  batch 14900 loss: 12.46213529586792\n",
      "  batch 15000 loss: 12.36149564743042\n",
      "  batch 15100 loss: 12.521456279754638\n",
      "  batch 15200 loss: 12.550866451263428\n",
      "  batch 15300 loss: 12.52062240600586\n",
      "  batch 15400 loss: 12.392136335372925\n",
      "  batch 15500 loss: 12.406949853897094\n",
      "  batch 15600 loss: 12.374104824066162\n",
      "  batch 15700 loss: 12.610321416854859\n",
      "  batch 15800 loss: 12.434752073287964\n",
      "  batch 15900 loss: 12.447848520278932\n",
      "  batch 16000 loss: 12.503573427200317\n",
      "  batch 16100 loss: 12.304195442199706\n",
      "  batch 16200 loss: 12.353659467697144\n",
      "  batch 16300 loss: 12.374157094955445\n",
      "  batch 16400 loss: 12.392096748352051\n",
      "  batch 16500 loss: 12.393438472747803\n",
      "  batch 16600 loss: 12.352947807312011\n",
      "  batch 16700 loss: 12.512567911148071\n",
      "  batch 16800 loss: 12.553472719192506\n",
      "  batch 16900 loss: 12.4196821975708\n",
      "  batch 17000 loss: 12.37692973136902\n",
      "  batch 17100 loss: 12.35384515762329\n",
      "  batch 17200 loss: 12.407268342971802\n",
      "  batch 17300 loss: 12.433023872375488\n",
      "  batch 17400 loss: 12.502993755340576\n",
      "  batch 17500 loss: 12.381325740814209\n",
      "  batch 17600 loss: 12.420222873687743\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS train 0.6585645931959152 valid 0.7715945243835449\n",
      "Epoch [1/1], Loss: 0.0038\n"
     ]
    }
   ],
   "source": [
    "from models import QuestionPairCosineSimilarity\n",
    "\n",
    "EPOCHS = 1\n",
    "LR = 1e-5\n",
    "\n",
    "cos_model = QuestionPairCosineSimilarity(len(result.vocab), result.embedding, 300, device)\n",
    "cos_model.to(device)\n",
    "\n",
    "# Logging setup\n",
    "timestamp = str(int(time.time()))\n",
    "fh = logging.FileHandler(LOG_DIR + timestamp + '_cos.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(fh)\n",
    "\n",
    "log(logger, 'Run timestamp: ' + timestamp)\n",
    "log(logger, f'EPOCHS: {EPOCHS}')\n",
    "log(logger, f'LR = {LR}')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cos_model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    cos_model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # Get inputs and labels\n",
    "        q1, q2, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cos_model(q1, q2)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * q1.size(0)\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            log(logger, '  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    running_vloss = 0.\n",
    "    \n",
    "    cos_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_dataloader):\n",
    "            vq1, vq2, vlabels = vdata\n",
    "            voutputs = cos_model(vq1, vq2)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    log(logger, 'LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    log(logger, f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text parameter is not string, but <class 'float'>: nan\n",
      "0.32549654950654233\n"
     ]
    }
   ],
   "source": [
    "running_tloss = 0.\n",
    "\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "cos_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(test_dataloader):\n",
    "        tq1, tq2, tlabels = tdata\n",
    "        toutputs = cos_model(tq1, tq2)\n",
    "        prob_toutputs = nn.functional.softmax(toutputs, dim=1)\n",
    "        prediction = torch.zeros_like(prob_toutputs)\n",
    "        mask = prob_toutputs > 0.5\n",
    "        prediction[mask] = 1.\n",
    "        prediction = prediction[:, 0]\n",
    "        correct_preds += int(torch.sum((prediction == tlabels) * (prediction == 1.)).float())\n",
    "        total_preds += prediction.size(0)\n",
    "        tloss = criterion(toutputs, tlabels)\n",
    "        running_tloss += tloss\n",
    "total = i + 1\n",
    "avg_tloss = running_tloss / total\n",
    "accuracy = correct_preds / total_preds\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
