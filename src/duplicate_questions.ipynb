{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAR Project 2024 - Team Mojave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google colab prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment these lines if you are running the notebook from Google Colab\n",
    "#\n",
    "# !git clone -b feature/sbert https://github.com/opacicmarko/duplicate-question-identification.git\n",
    "# %cd duplicate-question-identification/src/data/\n",
    "# !wget https://sbert.net/datasets/quora-IR-dataset.zip\n",
    "# !unzip quora-IR-dataset.zip\n",
    "# !mv classification/* sbert/\n",
    "# %cd ..\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from utils import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR = './saved_models/'\n",
    "LOG_DIR = './logs/'\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the questions and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_glove_embeddings, load_and_embed_questions, get_max_len\n",
    "\n",
    "# Load embeddings and calculate the average embedding\n",
    "glove_embeddings, glove_avg_embedding = load_glove_embeddings('glove-wikipedia/glove.6B.300d.txt', calculate_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...DONE\n",
      "Generating vocabulary...DONE\n",
      "Extracting only embeddings required for vocabulary...DONE\n",
      "6662\n"
     ]
    }
   ],
   "source": [
    "result = load_and_embed_questions('data/train.csv', None, glove_embeddings, glove_avg_embedding)\n",
    "print(len(result.vocab.idx_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6662\n",
      "[-3.6886e-01  1.6665e-01  5.3452e-02  3.1654e-01 -1.5587e-01  3.7323e-01\n",
      "  2.3476e-02 -7.7873e-02 -1.2866e-01 -1.4157e+00  4.2323e-01  1.0872e-01\n",
      "  1.9566e-01 -3.6958e-02 -3.1716e-01  3.2313e-02 -1.0706e-01  1.5411e-01\n",
      " -1.1385e-01 -5.3492e-01 -8.8111e-02 -8.2342e-02 -1.1802e-02  5.7562e-02\n",
      " -2.5138e-01 -2.3305e-01 -1.4013e-01 -2.9804e-01  4.2330e-01  1.4148e-01\n",
      "  2.1174e-01  3.0089e-03 -7.2124e-02  1.0726e-01 -1.4072e+00 -3.6393e-01\n",
      " -7.0648e-02 -3.5023e-02  9.2109e-03 -4.6376e-03  3.6941e-01 -3.1950e-01\n",
      " -4.0905e-01  8.1857e-02 -2.7848e-01 -2.3384e-01 -3.5798e-01  4.4094e-01\n",
      " -1.9289e-01  1.8111e-01  1.8372e-01 -1.7586e-01 -8.4658e-02  2.7628e-02\n",
      "  2.4343e-01  2.2452e-01  9.2825e-02  1.8590e-01 -1.6946e-01 -2.1119e-01\n",
      " -4.8065e-01  2.3327e-02  3.2214e-01 -4.8864e-01 -4.5548e-02 -6.6230e-01\n",
      " -8.0392e-02  1.7008e-01 -8.6406e-02 -1.7071e-01  3.8116e-02  3.6805e-01\n",
      " -9.9032e-02  1.8643e-01 -2.5289e-02 -3.4840e-01  2.6928e-01 -4.0263e-01\n",
      " -4.1668e-01  4.0442e-01 -3.0187e-03 -1.2581e-01  1.8271e-01  3.7474e-01\n",
      "  5.3416e-01  2.0162e-01 -3.3906e-01  2.2156e-01 -2.9287e-03  2.0424e-01\n",
      " -9.7543e-04 -4.0551e-02 -4.5608e-01 -4.3217e-02 -3.1431e-01  1.3197e-01\n",
      " -2.8520e-01  1.4870e-01 -1.1564e-01 -3.4639e-02 -4.8005e-02  9.5907e-02\n",
      "  8.2946e-02 -9.0491e-02 -3.1543e-01 -3.6012e-01 -1.8588e-01  3.1254e-01\n",
      " -4.6717e-01 -1.1390e-01 -1.6014e-01 -3.3473e-01 -5.0582e-01 -4.4279e-01\n",
      " -1.1597e-01 -1.9440e-01  1.0958e-01  8.9686e-02  1.9664e-01 -3.3752e-01\n",
      " -2.3088e-01 -2.8500e-01 -1.4345e-01  3.4356e-01  3.7536e-01  1.7423e-01\n",
      "  9.5387e-02  1.2255e-01  3.7534e-01  1.9590e-01 -2.0763e-01  4.2285e-01\n",
      "  4.7908e-02  2.8730e-03 -1.0248e-01 -1.0990e-02  3.0316e-02  3.0923e-01\n",
      " -7.8191e-02  1.7161e-01  1.2297e-01  1.1334e-01  5.8435e-02  7.7761e-02\n",
      " -4.3548e-01  1.9657e-01 -1.8545e-01  3.0334e-02  2.0523e-01  2.3109e-01\n",
      "  7.0326e-01 -6.1502e-02 -2.8793e-02  3.0059e-01  2.2463e-01 -2.3505e-01\n",
      " -8.8845e-01 -2.2767e-02 -4.9855e-01 -1.7016e-01 -1.3950e-01  2.5470e-02\n",
      " -1.6716e-01  1.5786e-01 -1.6651e-04 -2.8056e-02  1.3545e-01 -5.9520e-02\n",
      "  1.3900e-01  3.7920e-01  2.1455e-01  5.5587e-01 -6.4773e-01  3.0938e-02\n",
      " -2.2821e-02  1.7538e-01  1.4959e-01 -2.3300e-01 -1.7293e-01  1.0277e+00\n",
      "  1.7895e-01  3.0013e-01  1.7709e-02  8.8489e-02  3.7098e-01 -1.0803e-01\n",
      " -4.2111e-01  3.9310e-01 -2.1920e-01  2.0519e-01 -5.2980e-01 -2.5640e-01\n",
      " -1.1884e-01 -1.4738e-01  3.1549e-01 -1.3869e-01 -8.3845e-02  2.7897e-01\n",
      "  3.7843e-01 -1.1851e-02  1.2435e+00 -2.7210e-02 -1.6494e-01  6.8978e-02\n",
      " -1.4388e-02  1.0964e-01 -9.2518e-03  4.1301e-01  2.3176e-02 -2.7716e-01\n",
      "  2.2952e-01 -1.2557e-01  4.1034e-01 -2.4484e-01  9.3694e-02 -4.0638e-02\n",
      " -2.2682e-01  3.5462e-01 -5.9728e-02 -7.6283e-02  7.2801e-01 -2.3714e-01\n",
      " -5.9137e-01  8.4563e-01  2.3823e-02  1.3666e-01  1.8534e-01 -1.6146e-01\n",
      "  1.8006e-01  3.2675e-01 -4.4427e-02  2.7968e-02 -7.9990e-02 -4.3421e-01\n",
      "  3.0875e-02  3.0228e-02  4.8839e-01 -3.4265e-02  4.2164e-01 -1.5090e-01\n",
      " -2.2167e-01 -2.8621e-01  4.5150e-01  1.2549e-01 -1.0057e+00 -1.2271e-02\n",
      "  6.3056e-01 -6.5727e-02  7.7733e-03  3.9281e-02  1.1555e-01  3.0901e-02\n",
      " -1.9538e-02 -2.2340e-01  2.3992e-01 -2.4529e-01  3.1603e-01  2.1532e-01\n",
      "  4.0016e-01  1.5882e-01 -4.4926e-02  2.6241e-01 -3.0474e-02 -1.1219e-01\n",
      "  2.8185e-01 -1.3680e-02  1.8972e-01  3.2951e-02  3.6970e-01 -6.7917e-02\n",
      "  2.0682e-01  2.5769e-02  2.4305e-01  3.8442e-01  5.5733e-01 -2.9742e-01\n",
      " -2.2098e+00 -2.4059e-01  1.1628e-01  4.0257e-01 -6.1286e-01 -1.9978e-01\n",
      " -1.1517e-01  3.6771e-02 -1.4610e-01  7.3904e-01 -3.5831e-01 -9.7467e-02\n",
      " -4.6618e-01 -2.9867e-01  1.4423e-01  2.5053e-02  1.3952e-01  2.1374e-01\n",
      "  3.4057e-01  4.7053e-01  6.9752e-03 -2.4409e-01 -3.0849e-02 -3.1811e-02]\n"
     ]
    }
   ],
   "source": [
    "print(len(result.vocab.idx_words))\n",
    "\n",
    "# Sanity check (should look like [-0.36886 0.16665 0.053452 ... -0.030849 -0.031811])\n",
    "print(result.embedding[result.vocab.words_idx['year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating maximum question length...\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "Maximum length: 89\n"
     ]
    }
   ],
   "source": [
    "print('Calculating maximum question length...')\n",
    "max_len = get_max_len(pd.read_csv('data/train.csv'))\n",
    "print('Maximum length:', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasplit import make_dataset_split\n",
    "\n",
    "RANDOM_STATE = 73\n",
    "\n",
    "ORIGINAL_DATA_PATH = 'data/train.csv'\n",
    "TRAIN_PATH = 'data/mojave/mojave_train.csv'\n",
    "VALIDATION_PATH = 'data/mojave/mojave_validation.csv'\n",
    "TEST_PATH = 'data/mojave/mojave_test.csv'\n",
    "\n",
    "train_df, valid_df, test_df = make_dataset_split(\n",
    "    data_path=ORIGINAL_DATA_PATH,\n",
    "    train_path=TRAIN_PATH,\n",
    "    validation_path=VALIDATION_PATH,\n",
    "    test_path=TEST_PATH,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.plot.hist(by='is_duplicate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import ProcessedResult, preprocess_text\n",
    "\n",
    "def embedding_for_word(word: str, result: ProcessedResult) -> np.array:\n",
    "    idx = None\n",
    "    idx = result.vocab.words_idx.get(word, None)\n",
    "    if idx is None:\n",
    "        return result.avg_embedding\n",
    "    return result.embedding[idx]\n",
    "\n",
    "def question_embedding_tensor(text: str, result: ProcessedResult, device: torch.device) -> torch.Tensor:\n",
    "    tokens = preprocess_text(text)\n",
    "    token_embeddings = list(map(lambda token: embedding_for_word(token, result), tokens))\n",
    "    if len(token_embeddings) == 0:\n",
    "        token_embeddings = [np.array(result.avg_embedding)]\n",
    "    token_embeddings = np.vstack(token_embeddings)\n",
    "    return torch.from_numpy(token_embeddings.sum(axis=0)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset splits loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from preprocessing import MojaveVocab\n",
    "from typing import Any\n",
    "\n",
    "questions_dtype = {\n",
    "    'id': int,\n",
    "    'qid1': int,\n",
    "    'qid2': int,\n",
    "    'question1': str,\n",
    "    'question2': str,\n",
    "    'is_duplicate': int\n",
    "}\n",
    "\n",
    "class QuestionPairDataset(Dataset):\n",
    "    def __init__(self, questions_path: str, vocab: MojaveVocab, max_len: int, device: torch.device):\n",
    "        self.path = questions_path\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "        self.questions = pd.read_csv(questions_path, dtype=questions_dtype)\n",
    "        # self.result = load_and_embed_questions(questions_path, None, glove_embeddings, glove_avg_embedding)\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        row = self.questions.iloc[index]\n",
    "        question1 = preprocess_text(row['question1'])\n",
    "        question2 = preprocess_text(row['question2'])\n",
    "        label = row['is_duplicate']\n",
    "\n",
    "        question1_idxs = [self.vocab.words_idx.get(word, self.vocab.unk_idx) for word in question1]\n",
    "        question2_idxs = [self.vocab.words_idx.get(word, self.vocab.unk_idx) for word in question2]\n",
    "\n",
    "        question1_idxs = question1_idxs + [self.vocab.pad_idx] * (self.max_len - len(question1_idxs))\n",
    "        question2_idxs = question2_idxs + [self.vocab.pad_idx] * (self.max_len - len(question2_idxs))\n",
    "\n",
    "        return (\n",
    "            torch.tensor(question1_idxs, dtype=torch.long, device=self.device),\n",
    "            torch.tensor(question2_idxs, dtype=torch.long, device=self.device),\n",
    "            torch.tensor(label, dtype=torch.long, device=self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.backends.cuda.is_available() else 'cpu'\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "train_data = QuestionPairDataset(questions_path=TRAIN_PATH, vocab=result.vocab, max_len=max_len, device=device)\n",
    "validation_data = QuestionPairDataset(questions_path=VALIDATION_PATH, vocab=result.vocab, max_len=max_len, device=device)\n",
    "test_data = QuestionPairDataset(questions_path=TEST_PATH, vocab=result.vocab, max_len=max_len, device=device)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import QuestionPairMLP\n",
    "\n",
    "def train_one_epoch(model: QuestionPairMLP, criterion: nn.CrossEntropyLoss, optimizer: torch.optim.Optimizer, training_loader: DataLoader, device: torch.device):\n",
    "    validation_interval = 1000\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    for i, data in enumerate(training_loader):\n",
    "        question1, question2, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(question1, question2)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            log(logger, '  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "        if i % validation_interval == validation_interval - 1:\n",
    "            running_vloss = 0.0\n",
    "            # Set the model to evaluation mode, disabling dropout and using population\n",
    "            # statistics for batch normalization.\n",
    "            model.eval()\n",
    "\n",
    "            # Disable gradient computation and reduce memory consumption.\n",
    "            with torch.no_grad():\n",
    "                for i, vdata in enumerate(validation_dataloader):\n",
    "                    vq1, vq2, vlabels = vdata\n",
    "                    voutputs = model(vq1, vq2)\n",
    "                    vloss = criterion(voutputs, vlabels)\n",
    "                    running_vloss += vloss\n",
    "\n",
    "            avg_vloss = running_vloss / (i + 1)\n",
    "            log(logger, 'LOSS valid {}'.format(avg_vloss))\n",
    "        model.train(True)\n",
    "            \n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run at timestamp: 1716733115\n",
      "Total params: 85314\n",
      "HIDDEN_LAYER_SIZE_1: 128\n",
      "HIDDEN_LAYER_SIZE_2: 64\n",
      "LR = 0.0001\n",
      "WD = 0.1\n",
      "EPOCH 0\n",
      "  batch 100 loss: 0.6761941587924958\n",
      "  batch 200 loss: 0.6782734596729278\n",
      "  batch 300 loss: 0.6793051534891128\n",
      "  batch 400 loss: 0.6770786172151566\n",
      "  batch 500 loss: 0.6803968930244446\n",
      "  batch 600 loss: 0.6764743041992187\n",
      "  batch 700 loss: 0.677337054014206\n",
      "  batch 800 loss: 0.6756083422899246\n",
      "  batch 900 loss: 0.6778529155254364\n",
      "  batch 1000 loss: 0.6790100526809693\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.67635178565979\n",
      "  batch 1100 loss: 0.6800547695159912\n",
      "  batch 1200 loss: 0.6726067370176315\n",
      "  batch 1300 loss: 0.6738550990819931\n",
      "  batch 1400 loss: 0.6764506411552429\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "  batch 1500 loss: 0.674996309876442\n",
      "  batch 1600 loss: 0.6741819626092911\n",
      "  batch 1700 loss: 0.6740669453144074\n",
      "  batch 1800 loss: 0.6773726505041122\n",
      "  batch 1900 loss: 0.6780846983194351\n",
      "  batch 2000 loss: 0.6733337545394897\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6744663715362549\n",
      "  batch 2100 loss: 0.675062472820282\n",
      "  batch 2200 loss: 0.6755480182170868\n",
      "  batch 2300 loss: 0.6736624652147293\n",
      "  batch 2400 loss: 0.6722759133577347\n",
      "  batch 2500 loss: 0.6725102186203002\n",
      "  batch 2600 loss: 0.6712711381912232\n",
      "  batch 2700 loss: 0.6741603910923004\n",
      "  batch 2800 loss: 0.671235078573227\n",
      "  batch 2900 loss: 0.6721969252824783\n",
      "  batch 3000 loss: 0.6755654364824295\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6727758049964905\n",
      "  batch 3100 loss: 0.6752700459957123\n",
      "  batch 3200 loss: 0.6716873753070831\n",
      "  batch 3300 loss: 0.6717151767015457\n",
      "  batch 3400 loss: 0.6758524966239929\n",
      "  batch 3500 loss: 0.675411605834961\n",
      "  batch 3600 loss: 0.6752880698442459\n",
      "  batch 3700 loss: 0.6703368014097214\n",
      "  batch 3800 loss: 0.6758404719829559\n",
      "  batch 3900 loss: 0.6714435255527497\n",
      "  batch 4000 loss: 0.6751362770795822\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6714862585067749\n",
      "  batch 4100 loss: 0.6732707089185714\n",
      "  batch 4200 loss: 0.6745954090356827\n",
      "  batch 4300 loss: 0.6719033545255662\n",
      "  batch 4400 loss: 0.6686241567134857\n",
      "  batch 4500 loss: 0.6718905109167099\n",
      "  batch 4600 loss: 0.6727284896373749\n",
      "  batch 4700 loss: 0.6704559922218323\n",
      "  batch 4800 loss: 0.6681615829467773\n",
      "  batch 4900 loss: 0.6682741272449494\n",
      "  batch 5000 loss: 0.6667437100410462\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6701774001121521\n",
      "  batch 5100 loss: 0.671821762919426\n",
      "  batch 5200 loss: 0.6710418713092804\n",
      "  batch 5300 loss: 0.666299592256546\n",
      "  batch 5400 loss: 0.6704007518291474\n",
      "  batch 5500 loss: 0.6746987062692642\n",
      "  batch 5600 loss: 0.6651913160085678\n",
      "  batch 5700 loss: 0.6735317659378052\n",
      "  batch 5800 loss: 0.6691010755300522\n",
      "  batch 5900 loss: 0.6694383955001831\n",
      "  batch 6000 loss: 0.6668335288763046\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6690667271614075\n",
      "  batch 6100 loss: 0.66983118891716\n",
      "  batch 6200 loss: 0.6671609073877335\n",
      "  batch 6300 loss: 0.6670674771070481\n",
      "  batch 6400 loss: 0.669801470041275\n",
      "  batch 6500 loss: 0.6694205832481385\n",
      "  batch 6600 loss: 0.666103469133377\n",
      "  batch 6700 loss: 0.6629170906543732\n",
      "  batch 6800 loss: 0.6639631718397141\n",
      "  batch 6900 loss: 0.6681933516263961\n",
      "  batch 7000 loss: 0.6712328791618347\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6680063009262085\n",
      "  batch 7100 loss: 0.6669083219766617\n",
      "  batch 7200 loss: 0.6682381319999695\n",
      "  batch 7300 loss: 0.66974001288414\n",
      "  batch 7400 loss: 0.6671239423751831\n",
      "  batch 7500 loss: 0.669752242565155\n",
      "  batch 7600 loss: 0.6674215489625931\n",
      "  batch 7700 loss: 0.6712344837188721\n",
      "  batch 7800 loss: 0.6658094298839569\n",
      "  batch 7900 loss: 0.6679955917596817\n",
      "  batch 8000 loss: 0.6605110496282578\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6671226620674133\n",
      "  batch 8100 loss: 0.6673219764232635\n",
      "  batch 8200 loss: 0.6672303622961044\n",
      "  batch 8300 loss: 0.670825662612915\n",
      "  batch 8400 loss: 0.6716320520639419\n",
      "  batch 8500 loss: 0.6685687017440796\n",
      "  batch 8600 loss: 0.6703472995758056\n",
      "  batch 8700 loss: 0.6662319606542587\n",
      "  batch 8800 loss: 0.6651443940401077\n",
      "  batch 8900 loss: 0.6576725697517395\n",
      "  batch 9000 loss: 0.6673442900180817\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6663689613342285\n",
      "  batch 9100 loss: 0.6650218826532364\n",
      "  batch 9200 loss: 0.6697871041297913\n",
      "  batch 9300 loss: 0.6653816193342209\n",
      "  batch 9400 loss: 0.6628870970010757\n",
      "  batch 9500 loss: 0.6668062949180603\n",
      "  batch 9600 loss: 0.6679829978942871\n",
      "  batch 9700 loss: 0.6697051310539246\n",
      "  batch 9800 loss: 0.6632736575603485\n",
      "  batch 9900 loss: 0.666380888223648\n",
      "  batch 10000 loss: 0.6702668309211731\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.665717601776123\n",
      "  batch 10100 loss: 0.6634306597709656\n",
      "  batch 10200 loss: 0.664413338303566\n",
      "  batch 10300 loss: 0.6685180431604385\n",
      "  batch 10400 loss: 0.6679182833433152\n",
      "  batch 10500 loss: 0.6618445301055909\n",
      "  batch 10600 loss: 0.6610286253690719\n",
      "  batch 10700 loss: 0.6739817881584167\n",
      "  batch 10800 loss: 0.6636407476663589\n",
      "  batch 10900 loss: 0.6685829496383667\n",
      "  batch 11000 loss: 0.6661376112699509\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6651231646537781\n",
      "  batch 11100 loss: 0.6646259462833405\n",
      "  batch 11200 loss: 0.6660140228271484\n",
      "  batch 11300 loss: 0.6648433339595795\n",
      "  batch 11400 loss: 0.6642376345396042\n",
      "  batch 11500 loss: 0.6643737810850143\n",
      "  batch 11600 loss: 0.6648676282167435\n",
      "  batch 11700 loss: 0.6640570306777954\n",
      "  batch 11800 loss: 0.665519363284111\n",
      "  batch 11900 loss: 0.6683212047815323\n",
      "  batch 12000 loss: 0.6662021964788437\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6645748615264893\n",
      "  batch 12100 loss: 0.6673074156045914\n",
      "  batch 12200 loss: 0.6707444697618484\n",
      "  batch 12300 loss: 0.6684182798862457\n",
      "  batch 12400 loss: 0.6629606068134308\n",
      "  batch 12500 loss: 0.6685315907001496\n",
      "  batch 12600 loss: 0.6692826753854751\n",
      "  batch 12700 loss: 0.6640067571401596\n",
      "  batch 12800 loss: 0.6678712749481202\n",
      "  batch 12900 loss: 0.6686010348796845\n",
      "  batch 13000 loss: 0.6590075159072876\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6641659140586853\n",
      "  batch 13100 loss: 0.660093652009964\n",
      "  batch 13200 loss: 0.6598232120275498\n",
      "  batch 13300 loss: 0.6601280349493027\n",
      "  batch 13400 loss: 0.6602670150995255\n",
      "  batch 13500 loss: 0.6599772453308106\n",
      "  batch 13600 loss: 0.6615048795938492\n",
      "  batch 13700 loss: 0.6606446248292923\n",
      "  batch 13800 loss: 0.6680351901054382\n",
      "  batch 13900 loss: 0.6706520223617554\n",
      "  batch 14000 loss: 0.6718250584602355\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6636931300163269\n",
      "  batch 14100 loss: 0.6629411005973815\n",
      "  batch 14200 loss: 0.6663492947816849\n",
      "  batch 14300 loss: 0.664284166097641\n",
      "  batch 14400 loss: 0.6603764283657074\n",
      "  batch 14500 loss: 0.6680550014972687\n",
      "  batch 14600 loss: 0.668261656165123\n",
      "  batch 14700 loss: 0.6674140858650207\n",
      "  batch 14800 loss: 0.6692439883947372\n",
      "  batch 14900 loss: 0.6657367187738419\n",
      "  batch 15000 loss: 0.6583159220218658\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6633456349372864\n",
      "  batch 15100 loss: 0.662783260345459\n",
      "  batch 15200 loss: 0.6643996959924698\n",
      "  batch 15300 loss: 0.6610603660345078\n",
      "  batch 15400 loss: 0.6605712777376175\n",
      "  batch 15500 loss: 0.6655290216207504\n",
      "  batch 15600 loss: 0.6542170590162277\n",
      "  batch 15700 loss: 0.6612506258487701\n",
      "  batch 15800 loss: 0.6605634337663651\n",
      "  batch 15900 loss: 0.6594802814722062\n",
      "  batch 16000 loss: 0.6606751644611358\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.6629070043563843\n",
      "  batch 16100 loss: 0.6617107164859771\n",
      "  batch 16200 loss: 0.6591103744506835\n",
      "  batch 16300 loss: 0.665234904885292\n",
      "  batch 16400 loss: 0.6677799797058106\n",
      "  batch 16500 loss: 0.662412760257721\n",
      "  batch 16600 loss: 0.6673054671287537\n",
      "  batch 16700 loss: 0.656565037369728\n",
      "  batch 16800 loss: 0.6618994039297104\n",
      "  batch 16900 loss: 0.6579755955934524\n",
      "  batch 17000 loss: 0.6652549427747726\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS valid 0.662572979927063\n",
      "  batch 17100 loss: 0.6641696763038635\n",
      "  batch 17200 loss: 0.6645646953582763\n",
      "  batch 17300 loss: 0.660861748456955\n",
      "  batch 17400 loss: 0.6623380589485168\n",
      "  batch 17500 loss: 0.6618810623884201\n",
      "  batch 17600 loss: 0.6585645931959152\n",
      "DONE with training\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_LAYER_SIZE_1 = 128\n",
    "HIDDEN_LAYER_SIZE_2 = 64\n",
    "EPOCHS = 1\n",
    "LR = 1e-4\n",
    "WD = 0.1\n",
    "\n",
    "model = QuestionPairMLP(len(result.vocab), result.embedding, 300, HIDDEN_LAYER_SIZE_1, HIDDEN_LAYER_SIZE_2, device)\n",
    "model.to(device)\n",
    "\n",
    "# Logging setup\n",
    "timestamp = str(int(time.time()))\n",
    "fh = logging.FileHandler(LOG_DIR + timestamp + '_mlp.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(fh)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "log(logger, 'Run at timestamp: ' + timestamp)\n",
    "log(logger, f'Total params: {sum(parameter.numel() for parameter in model.parameters() if parameter.requires_grad)}')\n",
    "log(logger, f'HIDDEN_LAYER_SIZE_1: {HIDDEN_LAYER_SIZE_1}')\n",
    "log(logger, f'HIDDEN_LAYER_SIZE_2: {HIDDEN_LAYER_SIZE_2}')\n",
    "log(logger, f'LR = {LR}')\n",
    "log(logger, f'WD = {WD}')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    log(logger, f'EPOCH {epoch}')\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(model, criterion, optimizer, train_dataloader, device)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_DIR + timestamp + '.model')\n",
    "\n",
    "print('DONE with training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text parameter is not string, but <class 'float'>: nan\n"
     ]
    }
   ],
   "source": [
    "running_tloss = 0.\n",
    "\n",
    "correct_pred = 0\n",
    "total_pred = len(test_data)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(test_dataloader):\n",
    "        tq1, tq2, tlabels = tdata\n",
    "        toutputs = model(tq1, tq2)\n",
    "        prob_toutputs = nn.functional.softmax(toutputs, dim=1)\n",
    "        prediction = torch.zeros_like(prob_toutputs)\n",
    "        mask = toutputs > 0.5\n",
    "        prediction[mask] = 1.\n",
    "        prediction = prediction[:, 0]\n",
    "        correct_pred += int(torch.sum((prediction == tlabels) * (prediction == 1.)).float())\n",
    "        total_pred += prediction.size(0)\n",
    "        tloss = criterion(toutputs, tlabels)\n",
    "        running_tloss += tloss\n",
    "total = i + 1\n",
    "avg_tloss = running_tloss / total\n",
    "accuracy = correct_pred / total_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg_tloss\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run timestamp: 1716734587\n",
      "EPOCHS: 1\n",
      "LR = 1e-05\n",
      "  batch 100 loss: 13.125542163848877\n",
      "  batch 200 loss: 13.084531574249267\n",
      "  batch 300 loss: 13.01950047492981\n",
      "  batch 400 loss: 13.44168363571167\n",
      "  batch 500 loss: 13.15376296043396\n",
      "  batch 600 loss: 13.1065620803833\n",
      "  batch 700 loss: 13.172521514892578\n",
      "  batch 800 loss: 13.176909551620483\n",
      "  batch 900 loss: 13.132677526474\n",
      "  batch 1000 loss: 13.117112083435059\n",
      "  batch 1100 loss: 13.265741348266602\n",
      "  batch 1200 loss: 13.09733465194702\n",
      "  batch 1300 loss: 13.094244260787963\n",
      "  batch 1400 loss: 13.01409460067749\n",
      "  batch 1500 loss: 13.022502641677857\n",
      "  batch 1600 loss: 12.835916185379029\n",
      "  batch 1700 loss: 13.016829757690429\n",
      "  batch 1800 loss: 13.062206964492798\n",
      "  batch 1900 loss: 13.228605394363404\n",
      "  batch 2000 loss: 13.041410112380982\n",
      "  batch 2100 loss: 13.02677752494812\n",
      "  batch 2200 loss: 12.912434816360474\n",
      "  batch 2300 loss: 12.917502946853638\n",
      "  batch 2400 loss: 13.07784942626953\n",
      "  batch 2500 loss: 13.19842155456543\n",
      "  batch 2600 loss: 12.909504547119141\n",
      "  batch 2700 loss: 12.92250415802002\n",
      "  batch 2800 loss: 13.098742179870605\n",
      "  batch 2900 loss: 12.966028289794922\n",
      "  batch 3000 loss: 13.236049499511719\n",
      "  batch 3100 loss: 12.959516630172729\n",
      "  batch 3200 loss: 13.02415584564209\n",
      "  batch 3300 loss: 12.911162433624268\n",
      "  batch 3400 loss: 13.045729312896729\n",
      "  batch 3500 loss: 12.864895057678222\n",
      "  batch 3600 loss: 12.865869131088257\n",
      "  batch 3700 loss: 12.778618173599243\n",
      "  batch 3800 loss: 12.778739309310913\n",
      "  batch 3900 loss: 12.910044946670531\n",
      "  batch 4000 loss: 12.926288213729858\n",
      "  batch 4100 loss: 12.915350818634034\n",
      "  batch 4200 loss: 12.770366973876953\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "  batch 4300 loss: 12.928938083648681\n",
      "  batch 4400 loss: 12.777683477401734\n",
      "  batch 4500 loss: 12.897811460494996\n",
      "  batch 4600 loss: 12.98119213104248\n",
      "  batch 4700 loss: 12.9072332572937\n",
      "  batch 4800 loss: 12.911561145782471\n",
      "  batch 4900 loss: 12.793798608779907\n",
      "  batch 5000 loss: 12.791119108200073\n",
      "  batch 5100 loss: 12.87044140815735\n",
      "  batch 5200 loss: 12.798771572113036\n",
      "  batch 5300 loss: 12.69327223777771\n",
      "  batch 5400 loss: 12.685161533355712\n",
      "  batch 5500 loss: 12.758810148239135\n",
      "  batch 5600 loss: 12.851711902618408\n",
      "  batch 5700 loss: 12.870542573928834\n",
      "  batch 5800 loss: 12.938449869155884\n",
      "  batch 5900 loss: 12.691681089401245\n",
      "  batch 6000 loss: 12.786294708251953\n",
      "  batch 6100 loss: 12.870909509658814\n",
      "  batch 6200 loss: 12.717987718582153\n",
      "  batch 6300 loss: 12.772410459518433\n",
      "  batch 6400 loss: 12.684014940261841\n",
      "  batch 6500 loss: 12.791578168869018\n",
      "  batch 6600 loss: 12.883257083892822\n",
      "  batch 6700 loss: 12.781135540008545\n",
      "  batch 6800 loss: 12.757438707351685\n",
      "  batch 6900 loss: 12.785095777511597\n",
      "  batch 7000 loss: 12.699850521087647\n",
      "  batch 7100 loss: 12.729167642593383\n",
      "  batch 7200 loss: 12.919949750900269\n",
      "  batch 7300 loss: 13.009974575042724\n",
      "  batch 7400 loss: 12.807009258270263\n",
      "  batch 7500 loss: 12.803487005233764\n",
      "  batch 7600 loss: 12.845147180557252\n",
      "  batch 7700 loss: 12.722933931350708\n",
      "  batch 7800 loss: 12.673867263793944\n",
      "  batch 7900 loss: 12.706573543548584\n",
      "  batch 8000 loss: 12.745581884384155\n",
      "  batch 8100 loss: 12.746311597824096\n",
      "  batch 8200 loss: 12.639630126953126\n",
      "  batch 8300 loss: 12.780764141082763\n",
      "  batch 8400 loss: 12.653295783996581\n",
      "  batch 8500 loss: 12.6042138671875\n",
      "  batch 8600 loss: 12.577718925476074\n",
      "  batch 8700 loss: 12.738542127609254\n",
      "  batch 8800 loss: 12.616404151916504\n",
      "  batch 8900 loss: 12.655012826919556\n",
      "  batch 9000 loss: 12.58242247581482\n",
      "  batch 9100 loss: 12.625362596511842\n",
      "  batch 9200 loss: 12.677927436828613\n",
      "  batch 9300 loss: 12.41597785949707\n",
      "  batch 9400 loss: 12.493468608856201\n",
      "  batch 9500 loss: 12.684014625549317\n",
      "  batch 9600 loss: 12.61446816444397\n",
      "  batch 9700 loss: 12.729063606262207\n",
      "  batch 9800 loss: 12.592914056777953\n",
      "  batch 9900 loss: 12.609777736663819\n",
      "  batch 10000 loss: 12.49106580734253\n",
      "  batch 10100 loss: 12.607328453063964\n",
      "  batch 10200 loss: 12.515122451782226\n",
      "  batch 10300 loss: 12.447818155288696\n",
      "  batch 10400 loss: 12.586279544830322\n",
      "  batch 10500 loss: 12.71947639465332\n",
      "  batch 10600 loss: 12.64928568840027\n",
      "  batch 10700 loss: 12.688489255905152\n",
      "  batch 10800 loss: 12.581412200927735\n",
      "  batch 10900 loss: 12.632698793411254\n",
      "  batch 11000 loss: 12.58159670829773\n",
      "  batch 11100 loss: 12.680947494506835\n",
      "  batch 11200 loss: 12.689563436508179\n",
      "  batch 11300 loss: 12.506831483840942\n",
      "  batch 11400 loss: 12.518534460067748\n",
      "  batch 11500 loss: 12.538668146133423\n",
      "  batch 11600 loss: 12.524470901489257\n",
      "  batch 11700 loss: 12.284627256393433\n",
      "  batch 11800 loss: 12.50344488143921\n",
      "  batch 11900 loss: 12.588345994949341\n",
      "  batch 12000 loss: 12.64077838897705\n",
      "  batch 12100 loss: 12.466832647323608\n",
      "  batch 12200 loss: 12.489009971618652\n",
      "  batch 12300 loss: 12.596124305725098\n",
      "  batch 12400 loss: 12.566858625411987\n",
      "  batch 12500 loss: 12.353911380767823\n",
      "  batch 12600 loss: 12.54045599937439\n",
      "  batch 12700 loss: 12.525763549804687\n",
      "  batch 12800 loss: 12.449924564361572\n",
      "  batch 12900 loss: 12.518759031295776\n",
      "  batch 13000 loss: 12.561452808380126\n",
      "  batch 13100 loss: 12.589726085662841\n",
      "  batch 13200 loss: 12.337389526367188\n",
      "  batch 13300 loss: 12.566959819793702\n",
      "  batch 13400 loss: 12.57543249130249\n",
      "  batch 13500 loss: 12.644379444122315\n",
      "  batch 13600 loss: 12.449274854660034\n",
      "  batch 13700 loss: 12.76632761001587\n",
      "  batch 13800 loss: 12.667159872055054\n",
      "  batch 13900 loss: 12.568641729354859\n",
      "  batch 14000 loss: 12.607402572631836\n",
      "  batch 14100 loss: 12.361354942321777\n",
      "  batch 14200 loss: 12.615060539245606\n",
      "  batch 14300 loss: 12.485607042312623\n",
      "  batch 14400 loss: 12.520207796096802\n",
      "  batch 14500 loss: 12.423586263656617\n",
      "  batch 14600 loss: 12.506347589492798\n",
      "  batch 14700 loss: 12.519100894927979\n",
      "  batch 14800 loss: 12.316375532150268\n",
      "  batch 14900 loss: 12.46213529586792\n",
      "  batch 15000 loss: 12.36149564743042\n",
      "  batch 15100 loss: 12.521456279754638\n",
      "  batch 15200 loss: 12.550866451263428\n",
      "  batch 15300 loss: 12.52062240600586\n",
      "  batch 15400 loss: 12.392136335372925\n",
      "  batch 15500 loss: 12.406949853897094\n",
      "  batch 15600 loss: 12.374104824066162\n",
      "  batch 15700 loss: 12.610321416854859\n",
      "  batch 15800 loss: 12.434752073287964\n",
      "  batch 15900 loss: 12.447848520278932\n",
      "  batch 16000 loss: 12.503573427200317\n",
      "  batch 16100 loss: 12.304195442199706\n",
      "  batch 16200 loss: 12.353659467697144\n",
      "  batch 16300 loss: 12.374157094955445\n",
      "  batch 16400 loss: 12.392096748352051\n",
      "  batch 16500 loss: 12.393438472747803\n",
      "  batch 16600 loss: 12.352947807312011\n",
      "  batch 16700 loss: 12.512567911148071\n",
      "  batch 16800 loss: 12.553472719192506\n",
      "  batch 16900 loss: 12.4196821975708\n",
      "  batch 17000 loss: 12.37692973136902\n",
      "  batch 17100 loss: 12.35384515762329\n",
      "  batch 17200 loss: 12.407268342971802\n",
      "  batch 17300 loss: 12.433023872375488\n",
      "  batch 17400 loss: 12.502993755340576\n",
      "  batch 17500 loss: 12.381325740814209\n",
      "  batch 17600 loss: 12.420222873687743\n",
      "text parameter is not string, but <class 'float'>: nan\n",
      "LOSS train 0.6585645931959152 valid 0.7715945243835449\n",
      "Epoch [1/1], Loss: 0.0038\n"
     ]
    }
   ],
   "source": [
    "from models import QuestionPairCosineSimilarity\n",
    "\n",
    "EPOCHS = 1\n",
    "LR = 1e-5\n",
    "\n",
    "cos_model = QuestionPairCosineSimilarity(len(result.vocab), result.embedding, 300, device)\n",
    "cos_model.to(device)\n",
    "\n",
    "# Logging setup\n",
    "timestamp = str(int(time.time()))\n",
    "fh = logging.FileHandler(LOG_DIR + timestamp + '_cos.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(fh)\n",
    "\n",
    "log(logger, 'Run timestamp: ' + timestamp)\n",
    "log(logger, f'EPOCHS: {EPOCHS}')\n",
    "log(logger, f'LR = {LR}')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cos_model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    cos_model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # Get inputs and labels\n",
    "        q1, q2, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cos_model(q1, q2)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * q1.size(0)\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            log(logger, '  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    running_vloss = 0.\n",
    "    \n",
    "    cos_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_dataloader):\n",
    "            vq1, vq2, vlabels = vdata\n",
    "            voutputs = cos_model(vq1, vq2)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    log(logger, 'LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    log(logger, f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text parameter is not string, but <class 'float'>: nan\n",
      "0.32549654950654233\n"
     ]
    }
   ],
   "source": [
    "running_tloss = 0.\n",
    "\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "cos_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(test_dataloader):\n",
    "        tq1, tq2, tlabels = tdata\n",
    "        toutputs = cos_model(tq1, tq2)\n",
    "        prob_toutputs = nn.functional.softmax(toutputs, dim=1)\n",
    "        prediction = torch.zeros_like(prob_toutputs)\n",
    "        mask = prob_toutputs > 0.5\n",
    "        prediction[mask] = 1.\n",
    "        prediction = prediction[:, 0]\n",
    "        correct_preds += int(torch.sum((prediction == tlabels) * (prediction == 1.)).float())\n",
    "        total_preds += prediction.size(0)\n",
    "        tloss = criterion(toutputs, tlabels)\n",
    "        running_tloss += tloss\n",
    "total = i + 1\n",
    "avg_tloss = running_tloss / total\n",
    "accuracy = correct_preds / total_preds\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# import logging\n",
    "# import time\n",
    "\n",
    "# Suponiendo que result, LOG_DIR, train_dataloader, validation_dataloader y test_dataloader estn definidos\n",
    "\n",
    "from models import QuestionPairLSTM\n",
    "\n",
    "EPOCHS = 1\n",
    "LR = 1e-5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lstm_model = QuestionPairLSTM(len(result.vocab), result.embedding, embedding_size=300, hidden_size=128, num_layers=2, device=device)\n",
    "lstm_model.to(device)\n",
    "\n",
    "# Logging setup\n",
    "timestamp = str(int(time.time()))\n",
    "fh = logging.FileHandler(LOG_DIR + timestamp + '_lstm.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(fh)\n",
    "\n",
    "def log(logger, message):\n",
    "    logger.debug(message)\n",
    "    print(message)\n",
    "\n",
    "log(logger, 'Run timestamp: ' + timestamp)\n",
    "log(logger, f'EPOCHS: {EPOCHS}')\n",
    "log(logger, f'LR = {LR}')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    lstm_model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        q1, q2, labels = data\n",
    "        q1, q2, labels = q1.to(device), q2.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(q1, q2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * q1.size(0)\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100\n",
    "            log(logger, f'  batch {i + 1} loss: {last_loss}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_dataloader):\n",
    "            vq1, vq2, vlabels = vdata\n",
    "            vq1, vq2, vlabels = vq1.to(device), vq2.to(device), vlabels.to(device)\n",
    "            voutputs = lstm_model(vq1, vq2)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss.item() * vq1.size(0)\n",
    "\n",
    "    avg_vloss = running_vloss / len(validation_dataloader.dataset)\n",
    "    log(logger, f'LOSS train {running_loss / len(train_dataloader.dataset)} valid {avg_vloss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_tloss = 0.0\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(test_dataloader):\n",
    "        tq1, tq2, tlabels = tdata\n",
    "        tq1, tq2, tlabels = tq1.to(device), tq2.to(device), tlabels.to(device)\n",
    "        toutputs = lstm_model(tq1, tq2)\n",
    "        prob_toutputs = nn.functional.softmax(toutputs, dim=1)\n",
    "        _, predicted = torch.max(prob_toutputs, 1)\n",
    "        correct_preds += (predicted == tlabels).sum().item()\n",
    "        total_preds += tlabels.size(0)\n",
    "        tloss = criterion(toutputs, tlabels)\n",
    "        running_tloss += tloss.item() * tq1.size(0)\n",
    "\n",
    "avg_tloss = running_tloss / len(test_dataloader.dataset)\n",
    "accuracy = correct_preds / total_preds\n",
    "log(logger, f'Test Loss: {avg_tloss}, Test Accuracy: {accuracy}')\n",
    "print(f'Final Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers.readers import InputExample\n",
    "\n",
    "sbert_questions_dtype = {\n",
    "    'qid1': int,\n",
    "    'qid2': int,\n",
    "    'question1': str,\n",
    "    'question2': str,\n",
    "    'is_duplicate': int\n",
    "}\n",
    "\n",
    "class SBERTQuestionPairDataset(Dataset):\n",
    "    def __init__(self, questions_path: str):\n",
    "        self.path = questions_path\n",
    "        self.questions = pd.read_csv(questions_path, sep='\\t', dtype=sbert_questions_dtype, quoting=3)\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    def __getitem__(self, index) -> InputExample:\n",
    "        row = self.questions.iloc[index]\n",
    "        return InputExample(texts=[str(row['question1']), str(row['question2'])], label=int(row['is_duplicate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = SBERTQuestionPairDataset('data/sbert/train_pairs.tsv')\n",
    "dev_dataset = SBERTQuestionPairDataset('data/sbert/dev_pairs.tsv')\n",
    "test_dataset = SBERTQuestionPairDataset('data/sbert/test_pairs.tsv')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilled RoBERTa Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "import math\n",
    "\n",
    "NUM_EPOCHS = 4\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.set_per_process_memory_fraction(0.0)\n",
    "\n",
    "evaluator = CEBinaryClassificationEvaluator.from_input_examples(list(dev_dataset), name=\"QuoraQuestionPairs-dev\")\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader) * NUM_EPOCHS * WARMUP_RATIO)\n",
    "log(logger, \"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "model = CrossEncoder(\"distilroberta-base\", num_labels=1)\n",
    "\n",
    "timestamp = str(int(time.time()))\n",
    "\n",
    "MODEL_SAVE_PATH = MODEL_SAVE_DIR + timestamp + '/'\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    evaluation_steps=5000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=MODEL_SAVE_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "\n",
    "model = CrossEncoder(MODEL_SAVE_PATH)\n",
    "\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(list(test_dataset), name=\"QuoraQuestionPairs-test\")\n",
    "evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = SBERTQuestionPairDataset('data/sbert/train_pairs.tsv')\n",
    "dev_dataset = SBERTQuestionPairDataset('data/sbert/dev_pairs.tsv')\n",
    "test_dataset = SBERTQuestionPairDataset('data/sbert/test_pairs.tsv')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "import math\n",
    "\n",
    "NUM_EPOCHS = 4\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.set_per_process_memory_fraction(0.0)\n",
    "\n",
    "evaluator = CEBinaryClassificationEvaluator.from_input_examples(list(dev_dataset), name=\"QuoraQuestionPairs-dev\")\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader) * NUM_EPOCHS * WARMUP_RATIO)\n",
    "log(logger, \"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "model = CrossEncoder(\"roberta-base\", num_labels=1)\n",
    "\n",
    "timestamp = str(int(time.time()))\n",
    "\n",
    "MODEL_SAVE_PATH = MODEL_SAVE_DIR + timestamp + '/'\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    evaluation_steps=5000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=MODEL_SAVE_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "\n",
    "model = CrossEncoder(MODEL_SAVE_PATH)\n",
    "\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(list(test_dataset), name=\"QuoraQuestionPairs-test\")\n",
    "evaluator(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
